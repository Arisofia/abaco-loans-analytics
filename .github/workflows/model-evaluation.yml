name: Model Evaluation Pipeline
permissions:
  contents: read
  pull-requests: write

on:
  push:
    branches: [main, develop]
  pull_request:
    types: [opened, synchronize, reopened]
  schedule:
    - cron: '0 2 * * *'

jobs:
  evaluate-models:
    name: Evaluate ML Models
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r apps/analytics/requirements.txt
          pip install pytest-html pytest-json-report allure-pytest

      - name: Run evaluation tests
        continue-on-error: true
        run: |
          mkdir -p reports
          pytest tests/evaluation/ \
            --html=reports/evaluation-report.html \
            --self-contained-html \
            --json-report \
            --json-report-file=reports/evaluation-metrics.json \
            -v || echo "Evaluation tests completed"

      - name: Generate metrics visualization
        if: always()
        continue-on-error: true
        run: |
          mkdir -p reports/visualizations
          python scripts/evaluation/generate_visualizations.py \
            --metrics-file reports/evaluation-metrics.json \
            --output-dir reports/visualizations/ \
            || echo "Visualization generation skipped"

      - name: Check evaluation thresholds
        if: always()
        continue-on-error: true
        id: threshold-check
        run: |
          mkdir -p config
          touch config/evaluation-thresholds.yml
          python scripts/evaluation/check_thresholds.py \
            --metrics-file reports/evaluation-metrics.json \
            --config config/evaluation-thresholds.yml \
            --output threshold-results.json \
            || echo "Threshold check skipped"

      - name: Upload evaluation reports
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports
          path: |
            reports/
            threshold-results.json
          retention-days: 30
          if-no-files-found: ignore

      - name: Comment on PR with results
        if: github.event_name == 'pull_request' && always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = '## üìä Model Evaluation Results\n\n';

            try {
              if (fs.existsSync('reports/evaluation-metrics.json')) {
                const metricsData = fs.readFileSync('reports/evaluation-metrics.json', 'utf8');
                const metrics = JSON.parse(metricsData);
                body += '### Metrics Summary\n' +
                  `- **Accuracy**: ${metrics.accuracy || 'N/A'}\n` +
                  `- **Precision**: ${metrics.precision || 'N/A'}\n` +
                  `- **Recall**: ${metrics.recall || 'N/A'}\n` +
                  `- **F1 Score**: ${metrics.f1_score || 'N/A'}\n\n`;
              } else {
                body += '### Metrics Summary\n‚ö†Ô∏è No metrics file generated. Tests may not have run.\n\n';
              }
            } catch (e) {
              body += '### Metrics Summary\n‚ö†Ô∏è Failed to parse metrics: ' + e.message + '\n\n';
            }

            try {
              if (fs.existsSync('threshold-results.json')) {
                const thresholdData = fs.readFileSync('threshold-results.json', 'utf8');
                const thresholds = JSON.parse(thresholdData);
                body += '### Threshold Status\n' +
                  `${thresholds.passed ? '‚úÖ All thresholds passed' : '‚ùå Some thresholds failed'}\n\n`;
              }
            } catch (e) {
              body += '### Threshold Status\n‚ö†Ô∏è Could not verify thresholds\n\n';
            }

            body += `[View detailed report](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})`;

            try {
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            } catch (e) {
              console.log('Could not post comment: ' + e.message);
            }

      - name: Send Slack notification on failure
        if: always() && failure() && secrets.SLACK_WEBHOOK_URL != ''
        continue-on-error: true
        uses: slackapi/slack-github-action@v2
        with:
          webhook-type: INCOMING_WEBHOOK
          payload: |
            {
              "text": "üö® Model Evaluation Pipeline Failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Model Evaluation Pipeline Failed*\n*Repository:* ${{ github.repository }}\n*Branch:* ${{ github.ref_name }}\n*Commit:* `${{ github.sha }}`\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Details>"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
