name: Model Evaluation Pipeline
permissions:
  contents: read
  pull-requests: write

on:
  push:
    branches: [main, develop]
  pull_request:
    types: [opened, synchronize, reopened]
  schedule:
    - cron: '0 2 * * *'

jobs:
  evaluate-models:
    name: Evaluate ML Models
    runs-on: ubuntu-latest
    env:
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r apps/analytics/requirements.txt
          pip install pytest-html pytest-json-report allure-pytest

      - name: Run evaluation tests
        continue-on-error: true
        run: |
          mkdir -p reports
          pytest tests/evaluation/ \
            --html=reports/evaluation-report.html \
            --self-contained-html \
            --json-report \
            --json-report-file=reports/evaluation-metrics.json \
            -v || echo "Evaluation tests completed"

      - name: Generate metrics visualization
        if: always()
        continue-on-error: true
        run: |
          mkdir -p reports/visualizations
          python scripts/evaluation/generate_visualizations.py \
            --metrics-file reports/evaluation-metrics.json \
            --output-dir reports/visualizations/ \
            || echo "Visualization generation skipped"

      - name: Check evaluation thresholds
        if: always()
        continue-on-error: true
        id: threshold-check
        run: |
          mkdir -p config
          touch config/evaluation-thresholds.yml
          python scripts/evaluation/check_thresholds.py \
            --metrics-file reports/evaluation-metrics.json \
            --config config/evaluation-thresholds.yml \
            --output threshold-results.json \
            || echo "Threshold check skipped"

      - name: Upload evaluation reports
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports
          path: |
            reports/
            threshold-results.json
          retention-days: 30
          if-no-files-found: ignore

      - name: Comment on PR with results
        if: github.event_name == 'pull_request' && always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = '## üìä Model Evaluation Results\n\n';

            try {
              if (fs.existsSync('reports/evaluation-metrics.json')) {
                try {
                  const metricsContent = fs.readFileSync('reports/evaluation-metrics.json', 'utf8');
                  const metrics = JSON.parse(metricsContent);
                  body += '### Metrics Summary\n' +
                    `- **Accuracy**: ${metrics.accuracy || 'N/A'}\n` +
                    `- **Precision**: ${metrics.precision || 'N/A'}\n` +
                    `- **Recall**: ${metrics.recall || 'N/A'}\n` +
                    `- **F1 Score**: ${metrics.f1_score || 'N/A'}\n\n`;
                } catch (parseError) {
                  body += '### Metrics Summary\nFailed to parse metrics file.\n\n';
                  console.error('JSON parse error:', parseError.message);
                }
              } else {
                body += '### Metrics Summary\nNo metrics generated in this run.\n\n';
              }

              if (fs.existsSync('threshold-results.json')) {
                try {
                  const thresholdContent = fs.readFileSync('threshold-results.json', 'utf8');
                  const thresholds = JSON.parse(thresholdContent);
                  body += '### Threshold Status\n' +
                    `${thresholds.passed ? '‚úÖ All thresholds passed' : '‚ùå Some thresholds failed'}\n\n`;
                } catch (parseError) {
                  console.error('Threshold parse error:', parseError.message);
                }
              }

              body += `[View detailed report](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})`;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            } catch (error) {
              console.error('Comment creation error:', error.message);
            }

      - name: Send Slack notification on failure
        if: failure() && env.SLACK_WEBHOOK_URL != ''
        continue-on-error: true
        uses: slackapi/slack-github-action@v1.24.0
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          payload: |
            {
              "text": "üö® Model Evaluation Failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Model Evaluation Pipeline Failed*\n*Repository:* ${{ github.repository }}\n*Branch:* ${{ github.ref_name }}\n*Commit:* `${{ github.sha }}`\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Details>"
                  }
                }
              ]
            }
