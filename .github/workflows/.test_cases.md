# GitHub Actions Workflows - Detailed Test Cases

---

## Category 1: Syntax & Schema Validation

### Test Case WF-SYN-001: YAML Syntax Validation
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- yamllint installed (`pip install yamllint`)
- All workflow files present in `.github/workflows/`

**Test Steps**:
1. Run yamllint against all workflow files
2. Capture output for syntax errors
3. Verify no syntax errors reported

**Data**: All 50+ `.yml` files in `.github/workflows/`

**Expected Result**: 
- Exit code 0
- No lines with "error" severity
- Output: "X file(s) passed"

---

### Test Case WF-SYN-002: GitHub Actions Schema Validation
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- actionlint installed (`brew install actionlint` or `npm install -g actionlint`)
- GitHub Actions schema available locally

**Test Steps**:
1. Run actionlint against `.github/workflows/`
2. Parse validation output
3. Check for schema violations

**Data**: All workflow files

**Expected Result**:
- No schema errors
- Return code 0
- All workflows conform to GitHub Actions specifications

---

### Test Case WF-SYN-003: actionlint Full Validation
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- actionlint configured
- Shellcheck installed for shell script validation

**Test Steps**:
1. Execute: `actionlint -format=json .github/workflows/`
2. Parse JSON output
3. Filter for errors (not warnings)

**Data**: All workflow files

**Expected Result**:
- JSON output contains `errors: []`
- No violations in workflow syntax
- All conditionals properly formatted

---

### Test Case WF-SYN-004: Duplicate Workflow Names Check
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Python 3.9+
- PyYAML installed

**Test Steps**:
1. Parse all workflow YAML files
2. Extract `name:` field from each
3. Check for duplicates
4. Generate report

**Data**: `name` field from all workflows

**Expected Result**:
- No duplicate workflow names
- All names are unique
- All names are descriptive (not empty)

---

### Test Case WF-SYN-005: Required 'on' Trigger Validation
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- YAML parser available
- All workflow files present

**Test Steps**:
1. For each workflow file:
   - Parse YAML
   - Check for `on:` key
   - Verify it's not empty

**Data**: All workflow files

**Expected Result**:
- 100% of workflows have `on:` key
- At least one trigger configured (push, pull_request, schedule, workflow_dispatch)
- No workflows with empty trigger section

---

### Test Case WF-SYN-006: Required 'jobs' Section Validation
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- YAML parsing capability
- Schema definitions loaded

**Test Steps**:
1. For each workflow:
   - Check presence of `jobs:` key
   - Verify at least one job defined
   - Check job structure validity

**Data**: All workflow files

**Expected Result**:
- All workflows have `jobs:` section
- Each job has unique name
- Each job follows GitHub Actions job structure

---

## Category 2: Workflow Structure & Execution

### Test Case WF-STRUCT-001: 'runs-on' Required Field
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All workflow files parsed
- Job definitions extracted

**Test Steps**:
1. For each job in each workflow:
   - Extract job definition
   - Check for `runs-on:` field
   - Verify valid runner (ubuntu-latest, macos-latest, windows-latest, etc.)

**Data**: All job definitions across workflows

**Expected Result**:
- 100% of jobs have `runs-on`
- All runner values are valid
- No jobs missing runner specification

---

### Test Case WF-STRUCT-002: Step uses/run Mutual Exclusivity
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All workflow steps parsed
- Step definitions extracted

**Test Steps**:
1. For each step in each job:
   - Check for `uses:` field
   - Check for `run:` field
   - Verify only one is present (not both, not neither)
   - Exception: composite actions may have both

**Data**: All step definitions

**Expected Result**:
- Every step has either `uses:` OR `run:` (not both)
- No steps missing both properties
- Error steps properly identified

---

### Test Case WF-STRUCT-003: Conditional Logic Syntax
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All conditional expressions identified
- GitHub Actions context functions defined

**Test Steps**:
1. For each `if:` statement:
   - Validate GitHub Actions context syntax (success(), failure(), etc.)
   - Check step output references (steps.STEP_ID.outputs.OUTPUT)
   - Verify boolean operators (&&, ||, !)
   - Check env variable references

**Parameters**:
- Test conditions: `${{ success() }}`, `${{ failure() }}`, `${{ always() }}`, `${{ steps.id.outputs.value == 'true' }}`

**Expected Result**:
- All `if:` statements follow valid GitHub Actions syntax
- No undefined step references
- No incorrect context usage

---

### Test Case WF-STRUCT-004: Step ID Uniqueness
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All steps with `id:` fields identified
- Job scope established

**Test Steps**:
1. For each job:
   - Extract all step IDs
   - Check for duplicates within job
   - Report any collisions

**Data**: All step `id:` fields, scoped by job

**Expected Result**:
- No duplicate step IDs within same job
- All referenced step IDs exist
- IDs are alphanumeric with hyphens only

---

### Test Case WF-STRUCT-005: Job Dependency Validation
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All job definitions extracted
- Dependency graph created

**Test Steps**:
1. For each job with `needs:` field:
   - Verify referenced jobs exist in same workflow
   - Check for circular dependencies
   - Validate dependency syntax

**Data**: `needs:` fields from all jobs

**Expected Result**:
- All job dependencies resolve to existing jobs
- No circular dependency chains
- Dependency syntax correct

---

## Category 3: Environment & Secrets Management

### Test Case WF-ENV-001: No Hardcoded Secrets
**Priority**: Critical | **Type**: Security | **Automation**: Yes

**Preconditions**:
- Secret patterns defined (API keys, tokens, passwords)
- All workflow files scanned
- Regex patterns for common secret formats

**Test Steps**:
1. Define secret patterns:
   - `(api[_-]?key|token|secret|password|credential).*:` (case-insensitive)
   - AWS key patterns: `AKIA[0-9A-Z]{16}`
   - Bearer tokens: `Bearer [A-Za-z0-9\-_=]+`
2. Scan all workflow files
3. Flag matches (excluding secret references like `${{ secrets.* }}`)
4. Verify matches are only in comments or examples

**Data**: All workflow YAML content

**Expected Result**:
- Zero hardcoded secrets found
- Only secret variable references (`${{ secrets.* }}`) present
- Comments with example secrets properly marked

---

### Test Case WF-ENV-002: Environment Variable Scoping
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Environment variable definitions identified
- Scope levels understood (workflow, job, step)

**Test Steps**:
1. For each `env:` section:
   - Identify scope level (top-level, job-level, step-level)
   - Verify variables available at appropriate scopes
   - Check for scope conflicts (duplicate names)

**Data**: All `env:` definitions

**Expected Result**:
- Variables scoped correctly
- No scope conflicts
- Variable precedence follows GitHub Actions rules

---

### Test Case WF-ENV-003: Secret Reference Syntax
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All secret references identified
- GitHub Actions syntax rules defined

**Test Steps**:
1. Find all `${{ secrets.* }}` references
2. Validate syntax: `${{ secrets.SECRET_NAME }}`
3. Check for common mistakes:
   - Missing `secrets.` prefix
   - Incorrect bracket matching
   - Invalid variable names

**Data**: All secret references

**Expected Result**:
- All secret references use correct syntax
- All secret names are uppercase with underscores
- No malformed secret references

---

### Test Case WF-ENV-004: Secret Sanitization Implementation
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Workflows using optional secrets identified
- Sanitization function defined

**Test Steps**:
1. Identify workflows checking secret availability
2. Verify sanitization function present (converts to lowercase, checks against placeholder values)
3. Test sanitization logic with sample inputs

**Parameters**:
- Test inputs: `""`, `"token"`, `"secret"`, `"CHANGEME"`, `"replace-me"`, `"valid-api-key-12345"`

**Expected Result**:
- Placeholder values sanitized to empty string
- Real values preserved
- Secret availability check prevents execution with invalid credentials

---

### Test Case WF-ENV-005: Conditional Checks for Optional Secrets
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Workflows with optional integrations identified
- Step conditional logic identified

**Test Steps**:
1. For each optional secret:
   - Find secret check step
   - Identify steps dependent on check
   - Verify `if:` condition references check output correctly
   - Verify skip/fallback behavior when secret unavailable

**Data**: Optional secrets (Notion API, Slack webhook, etc.)

**Expected Result**:
- Steps skip gracefully when optional secrets missing
- Conditional outputs referenced correctly
- Workflow completes successfully with or without optional secrets

---

## Category 4: Trigger Configuration

### Test Case WF-TRIG-001: Push Triggers Properly Defined
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All workflows with `push` trigger identified
- Branch/path filter rules understood

**Test Steps**:
1. For each `push` trigger:
   - Verify `branches:` or `branches-ignore:` is valid
   - Verify `paths:` or `paths-ignore:` is valid
   - Check branch names are real/expected
   - Verify path patterns are correct glob patterns

**Data**: All `push` trigger configurations

**Expected Result**:
- All push triggers properly formatted
- Branch patterns match repository branches
- Path filters use valid glob syntax

---

### Test Case WF-TRIG-002: Pull Request Triggers Properly Defined
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All workflows with `pull_request` trigger identified
- PR event types defined

**Test Steps**:
1. For each `pull_request` trigger:
   - Verify `types:` field if present (opened, synchronize, reopened, etc.)
   - Check `branches:` configuration
   - Check `paths:` configuration if present
   - Verify syntax validity

**Data**: All `pull_request` trigger configurations

**Expected Result**:
- All PR triggers properly formatted
- Event types are valid GitHub PR event types
- Filters correctly configured

---

### Test Case WF-TRIG-003: Schedule (Cron) Syntax Validation
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All workflows with `schedule` trigger identified
- Cron syntax validator available

**Test Steps**:
1. For each `schedule` entry:
   - Extract cron expression
   - Validate cron syntax (5 or 6 fields)
   - Check for realistic timing (no overlapping schedules)
   - Verify timezone handling if specified

**Parameters**:
- Cron examples: `"0 5 * * *"` (daily at 5 AM), `"0 13 * * *"` (daily at 1 PM), `"0 2 * * *"` (daily at 2 AM)

**Expected Result**:
- All cron expressions valid
- No schedule conflicts
- Times reasonable and distributed

---

### Test Case WF-TRIG-004: Workflow Dispatch Parameters Valid
**Priority**: Medium | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All workflows with `workflow_dispatch` identified
- Input parameter definitions extracted

**Test Steps**:
1. For workflows with `workflow_dispatch` inputs:
   - Verify input names are valid
   - Check input types (string, choice, boolean, environment)
   - Validate default values match type
   - Check for required vs optional inputs

**Data**: All `inputs:` definitions under `workflow_dispatch`

**Expected Result**:
- All dispatch inputs properly typed
- Default values valid
- Input references work in workflow

---

### Test Case WF-TRIG-005: Branch and Path Filters Valid
**Priority**: Medium | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All branch/path filters identified
- Glob pattern validator available

**Test Steps**:
1. For each branch filter:
   - Check branch names against repository branches
   - Validate glob patterns in path filters
   - Verify ignore patterns syntax
2. Test with sample paths/branches

**Data**: All `branches:`, `branches-ignore:`, `paths:`, `paths-ignore:` values

**Expected Result**:
- All filters valid and matching expected branches/paths
- Glob patterns correct
- No contradictory filters

---

## Category 5: CI Workflow Tests

### Test Case WF-CI-001: Web Build Succeeds
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Repository has `apps/web` directory
- Node.js and pnpm installed
- `package.json` files present

**Test Steps**:
1. Checkout main branch
2. Set up Node.js (version from workflow)
3. Install pnpm
4. Install dependencies: `pnpm install --frozen-lockfile`
5. Run build: `pnpm -C apps/web build`
6. Verify `.next` build directory created
7. Check for build errors in output

**Data**: Web application source code

**Expected Result**:
- Build completes successfully
- No TypeScript errors
- No build warnings (at warning level)
- `.next` directory created
- Exit code 0

---

### Test Case WF-CI-002: Lint Checks Pass
**Priority**: Critical | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- ESLint configured for web app
- Prettier configured
- Ruff configured for Python

**Test Steps**:
1. Set up development environment
2. Run ESLint: `pnpm -C apps/web lint` (or equivalent)
3. Run Prettier check (if applicable)
4. Collect violations

**Data**: Source code in `apps/web/src`, `src/`, `python/`

**Expected Result**:
- No ESLint errors
- No Prettier format violations
- No linting failures
- Exit code 0

---

### Test Case WF-CI-003: Type Checking Passes
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- TypeScript configured
- mypy configured for Python
- Type definitions present

**Test Steps**:
1. Run TypeScript type check: `tsc --noEmit` (or `pnpm typecheck`)
2. Run mypy for Python files: `mypy src/ --ignore-missing-imports`
3. Collect type errors
4. Verify strict mode passes

**Data**: TypeScript and Python source files

**Expected Result**:
- Zero type errors
- All type definitions valid
- Exit code 0 from both type checkers

---

### Test Case WF-CI-004: Test Coverage > 85%
**Priority**: High | **Type**: Performance | **Automation**: Yes

**Preconditions**:
- pytest configured
- Coverage tool installed
- Test suite defined

**Test Steps**:
1. Run tests with coverage: `pytest --cov=src --cov-report=xml --cov-report=term`
2. Extract coverage percentage
3. Compare against 85% threshold
4. Generate coverage report

**Data**: Test files in `tests/`

**Expected Result**:
- Coverage >= 85%
- All critical paths covered
- Coverage report generated
- Exit code 0

---

### Test Case WF-CI-005: Analytics Tests Execute
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Analytics test suite present
- Dependencies installed
- Test data available

**Test Steps**:
1. Install Python dependencies
2. Run analytics tests: `pytest tests/test_analytics.py -v`
3. Collect test results
4. Verify all tests pass

**Data**: Analytics test suite, sample data

**Expected Result**:
- All analytics tests pass
- No skipped tests
- Execution completes in < 10 minutes
- Exit code 0

---

## Category 6: Deployment Workflow Tests

### Test Case WF-DEPLOY-001: Vercel Secrets Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Secret sanitization function defined
- Vercel secrets configured (or test with missing secrets)

**Test Steps**:
1. Set Vercel secrets to test values
2. Run secret check step
3. Verify outputs:
   - `present=true` when all secrets present
   - `present=false` when any missing
4. Test with missing/invalid secrets

**Data**: Sanitized test values

**Expected Result**:
- Secret validation works correctly
- Outputs properly set
- Graceful handling of missing secrets

---

### Test Case WF-DEPLOY-002: AWS Secrets Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- AWS secret check step present
- Sanitization function available

**Test Steps**:
1. Test with all secrets present
2. Test with missing S3 bucket
3. Test with missing access keys
4. Verify appropriate output values

**Data**: AWS credentials (sanitized/test)

**Expected Result**:
- AWS validation works correctly
- `present` output accurate
- Workflow proceeds with or without AWS secrets

---

### Test Case WF-DEPLOY-003: Build Artifact Created
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Build completes successfully
- Artifact upload action configured

**Test Steps**:
1. Complete build process
2. Verify artifact created
3. Verify artifact contains expected files
4. Check artifact upload succeeds

**Data**: Build output, artifact files

**Expected Result**:
- Artifact created successfully
- Contains all necessary files
- Can be downloaded/restored
- Upload completes without error

---

### Test Case WF-DEPLOY-004: S3 Upload Succeeds (Partial)
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- AWS secrets available
- S3 bucket exists and accessible
- Build artifacts ready

**Test Steps**:
1. Verify AWS credentials configured
2. Run S3 sync: `aws s3 sync apps/web/.next s3://bucket/path --acl private`
3. Verify files uploaded
4. Check S3 bucket contents

**Data**: Build artifacts, AWS credentials

**Expected Result**:
- S3 upload succeeds
- All files have private ACL
- S3 contents match local build
- Exit code 0

**Manual Verification Required**: S3 bucket access and file verification

---

### Test Case WF-DEPLOY-005: Vercel Deployment Succeeds (Partial)
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Vercel credentials configured
- Vercel project exists
- Build artifacts ready

**Test Steps**:
1. Verify Vercel secrets present
2. Run Vercel deploy action
3. Monitor deployment progress
4. Verify deployment URL accessible

**Data**: Build artifacts, Vercel credentials

**Expected Result**:
- Deployment succeeds
- Deployment URL generated
- Application accessible at URL
- Environment variables applied

**Manual Verification Required**: URL accessibility, deployment completion

---

### Test Case WF-DEPLOY-006: Skip Gracefully When Secrets Missing
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Deploy workflow configured with secret checks
- Fallback steps defined

**Test Steps**:
1. Remove/unset Vercel secrets
2. Run workflow
3. Verify secret check outputs `present=false`
4. Verify "Skip deploy" step executes
5. Verify workflow completes with warning
6. Check that deployment steps are skipped

**Data**: Missing Vercel secrets

**Expected Result**:
- Workflow skips deployment gracefully
- No errors thrown
- Warning message displayed
- Exit code 0

---

## Category 7: Azure Dashboard Deployment

### Test Case WF-DASH-001: Azure Credentials Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Secret sanitization function present
- Azure credential format defined

**Test Steps**:
1. Test with valid Azure credentials
2. Test with missing credentials
3. Verify outputs:
   - `has_creds=true` when valid
   - `has_creds=false` when missing
4. Verify conditional steps respect this output

**Data**: Azure credential test values

**Expected Result**:
- Azure credential validation works
- Outputs correctly set
- Conditional logic follows outputs

---

### Test Case WF-DASH-002: Python Dependencies Installed
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Python 3.11 available
- requirements.txt present
- All dependencies in requirements.txt available

**Test Steps**:
1. Set up Python 3.11
2. Upgrade pip
3. Install requirements: `pip install -r requirements.txt`
4. Verify installation succeeds
5. Check no missing dependencies

**Data**: requirements.txt

**Expected Result**:
- pip upgrade succeeds
- All packages install without error
- No version conflicts
- Exit code 0

---

### Test Case WF-DASH-003: Core Files Exist Check
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Repository checked out
- Core files expected: requirements.txt, streamlit_app.py, startup.sh

**Test Steps**:
1. Test file existence:
   - `test -f requirements.txt`
   - `test -f streamlit_app.py`
   - `test -f startup.sh`
2. Verify all tests pass

**Data**: File paths

**Expected Result**:
- All three files exist
- Test command exit code 0
- Workflow proceeds to next step

---

### Test Case WF-DASH-004: Health Check Path Configuration (Partial)
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Azure credentials available
- Azure CLI installed
- Azure App Service exists

**Test Steps**:
1. Verify Azure login succeeds
2. Run Azure resource update command
3. Set health check path to `/?page=health`
4. Verify path set successfully

**Data**: Azure resource group, app name, health check path

**Expected Result**:
- Health check path configured
- Azure returns success response
- Configuration persists

**Manual Verification Required**: Azure portal confirmation

---

### Test Case WF-DASH-005: App Service Deployment Succeeds (Partial)
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Azure credentials configured
- Azure App Service exists and accessible
- Application code ready

**Test Steps**:
1. Run azure/webapps-deploy action
2. Deploy application package
3. Monitor deployment progress
4. Verify deployment completes

**Data**: App configuration, startup command

**Expected Result**:
- Deployment completes successfully
- No deployment errors
- Application ready at endpoint
- Startup command executes

**Manual Verification Required**: App Service deployment status

---

### Test Case WF-DASH-006: Post-Deployment Health Check Passes (Partial)
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Deployment completed
- Health check endpoint configured
- Application responsive

**Test Steps**:
1. Resolve App Service hostname
2. Retry health check up to 5 times (with 10s delay)
3. Verify curl to health check endpoint returns 200
4. Check response contains "ok"

**Data**: Health check URL

**Expected Result**:
- Health check passes (HTTP 200)
- Response contains "ok"
- Application is healthy and responsive
- Completes within 5 attempts (50 seconds)

**Manual Verification Required**: Application health endpoint verification

---

## Category 8: Data Ingestion & Analytics

### Test Case WF-INGEST-001: Cascade Credentials Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Secret sanitization function present
- Cascade credential format defined

**Test Steps**:
1. Test with valid credentials
2. Test with missing username
3. Test with missing password
4. Verify outputs:
   - `available=true` when both present
   - `available=false` when either missing

**Data**: Test Cascade credentials

**Expected Result**:
- Credential validation works
- Outputs correctly reflect credential status
- Conditional steps respect outputs

---

### Test Case WF-INGEST-002: Slack Webhook Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Secret sanitization present
- Slack webhook URL format defined

**Test Steps**:
1. Test with valid webhook URL
2. Test with missing webhook
3. Verify outputs:
   - `available=true` when present
   - `available=false` when missing

**Data**: Test Slack webhook URLs

**Expected Result**:
- Webhook validation works
- Outputs correctly set
- Notification step conditions respect this

---

### Test Case WF-INGEST-003: Python Dependencies Installed
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Python 3.11 available
- requirements.txt present

**Test Steps**:
1. Upgrade pip
2. Install requirements.txt
3. Verify no errors
4. Check all packages available

**Data**: requirements.txt, Python 3.11

**Expected Result**:
- Dependency installation succeeds
- All packages available for import
- Exit code 0

---

### Test Case WF-INGEST-004: Data Pipeline Executes
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Data pipeline script present
- Dependencies installed
- Data sources accessible

**Test Steps**:
1. Verify script exists: `[ -f scripts/run_data_pipeline.py ]`
2. Execute pipeline: `python scripts/run_data_pipeline.py`
3. Monitor for errors
4. Verify output data created

**Data**: Data source files, script

**Expected Result**:
- Pipeline executes successfully
- Output data files created
- No errors in execution
- Exit code 0

**Manual Verification Required**: Output data quality check

---

### Test Case WF-INGEST-005: Slack Notification Sent
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Slack webhook configured and valid
- Notification payload properly formatted
- Slack API accessible

**Test Steps**:
1. Verify webhook availability
2. Trigger Slack notification step
3. Monitor HTTP response from Slack
4. Verify notification delivered

**Data**: Slack webhook URL, notification payload

**Expected Result**:
- Notification sent successfully
- HTTP 200 response from Slack
- Message appears in target Slack channel
- Notification includes correct run details

**Manual Verification Required**: Slack channel message verification

---

### Test Case WF-INGEST-006: Skip Gracefully Without Secrets
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Workflow configured with secret checks
- Fallback steps defined

**Test Steps**:
1. Unset Cascade and Slack secrets
2. Run workflow
3. Verify warning issued
4. Verify scraper step skipped
5. Verify pipeline step runs if file exists
6. Verify workflow completes

**Data**: Workflow configuration

**Expected Result**:
- Workflow completes without errors
- Optional steps skipped gracefully
- Warnings logged appropriately
- Exit code 0

---

## Category 9: Growth Experiments & Meta Export

### Test Case WF-GROWTH-001: Secrets Validation Works
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Secret check step present
- Sanitization function defined

**Test Steps**:
1. Test with required secrets present
2. Test with missing OpenAI API key
3. Test with missing Supabase role
4. Verify outputs accordingly

**Data**: Test secret values

**Expected Result**:
- `run=true` when required secrets present
- `run=false` when any missing
- Optional Notion secrets tracked separately

---

### Test Case WF-META-001: Azure Storage Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Azure storage connection string format defined
- Sanitization function present

**Test Steps**:
1. Test with valid connection string
2. Test with missing connection string
3. Verify validation output
4. Verify conditional steps respect output

**Data**: Test Azure storage credentials

**Expected Result**:
- Validation works correctly
- Outputs set appropriately
- Conditional logic follows outputs

---

### Test Case WF-META-002: Meta API Token Validated
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Meta API token format defined
- Alternative Facebook token supported

**Test Steps**:
1. Test with Meta API token
2. Test with Facebook API token (fallback)
3. Test with missing both
4. Verify validation output

**Data**: Test API tokens

**Expected Result**:
- Either Meta or Facebook token accepted
- `can_run=true` with valid token
- `can_run=false` without either

---

### Test Case WF-META-004: TypeScript Compilation Succeeds
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- TypeScript files present in `node/services/meta-connector`
- Dependencies installed
- TypeScript compiler available

**Test Steps**:
1. Change to meta-connector directory
2. Compile TypeScript: `npx tsc`
3. Verify `dist/` directory created
4. Check for compilation errors

**Data**: TypeScript source files

**Expected Result**:
- Compilation succeeds with no errors
- `dist/` directory created
- JavaScript output files generated
- Exit code 0

---

## Category 10: KPI Parity & Model Evaluation

### Test Case WF-KPI-001: Database Availability Checked
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Database secret check step present
- Sanitization function defined

**Test Steps**:
1. Test with valid DATABASE_URL
2. Test with missing DATABASE_URL
3. Verify outputs:
   - `db_available=true` when present
   - `db_available=false` when missing

**Data**: Test DATABASE_URL values

**Expected Result**:
- Database validation works
- Outputs correctly reflect availability
- Subsequent steps respect this output

---

### Test Case WF-KPI-003: SQL Migrations Apply Correctly
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Database accessible via DATABASE_URL
- Migration file present: `supabase/migrations/20260101_analytics_kpi_views.sql`
- psql client available

**Test Steps**:
1. Verify database connection
2. Run migration: `psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f migration_file.sql`
3. Verify no errors
4. Verify views created in database

**Data**: Migration SQL file, database connection

**Expected Result**:
- Migration executes successfully
- Analytics views created
- No SQL errors
- Exit code 0

**Manual Verification Required**: Database view verification

---

### Test Case WF-KPI-004: KPI Parity Tests Pass
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Database accessible
- Test file exists: `tests/test_kpi_parity.py`
- Database has analytics views
- RUN_KPI_PARITY_TESTS=1 set

**Test Steps**:
1. Set RUN_KPI_PARITY_TESTS environment variable
2. Run pytest: `pytest -q tests/test_kpi_parity.py`
3. Verify all tests pass
4. Check test output for assertions

**Data**: KPI parity test suite

**Expected Result**:
- All tests pass
- Python KPI calculations match SQL views
- No assertion failures
- Exit code 0

**Manual Verification Required**: Test result analysis

---

### Test Case WF-MODEL-002: Evaluation Tests Execute
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Evaluation test suite present: `tests/evaluation/`
- Dependencies installed
- Test data available

**Test Steps**:
1. Create reports directory
2. Run pytest: `pytest tests/evaluation/ --html=reports/evaluation-report.html --json-report --json-report-file=reports/evaluation-metrics.json`
3. Verify tests execute
4. Check reports generated

**Data**: Evaluation test files

**Expected Result**:
- Tests execute successfully
- HTML report created
- JSON metrics file created
- Exit code 0 (continue-on-error allows failures)

---

### Test Case WF-MODEL-006: PR Comments Posted
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Running in PR context
- GitHub API token available
- Metrics files exist
- gh CLI available

**Test Steps**:
1. Verify metrics file exists
2. Parse metrics from JSON
3. Build comment body
4. Post comment to PR
5. Verify comment appears on PR

**Data**: Evaluation metrics, GitHub context

**Expected Result**:
- Comment successfully posted to PR
- Comment contains metrics summary
- Comment includes view details link
- PR comment visible to reviewers

**Manual Verification Required**: PR comment visibility

---

## Category 11: Perplexity Code Review

### Test Case WF-PERP-001: Secrets Availability Checked
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Secret check step present
- Sanitization function defined

**Test Steps**:
1. Test with valid Perplexity API key
2. Test with missing key
3. Verify outputs:
   - `secrets_available=true` when present
   - `secrets_available=false` when missing

**Data**: Test Perplexity API keys

**Expected Result**:
- Secret validation works
- Outputs correct
- Subsequent steps respect outputs

---

### Test Case WF-PERP-002: PR Diff Retrieved
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Running in PR context
- Git available
- PR branch accessible

**Test Steps**:
1. Checkout code
2. Get PR diff: `git diff origin/base...HEAD > pr_diff.txt`
3. Verify diff file created
4. Check diff contains changes

**Data**: PR changes

**Expected Result**:
- Diff file created successfully
- Contains PR code changes
- File size calculated and output
- Diff size < 50000 characters (for API call)

---

### Test Case WF-PERP-003: Diff Size Validation
**Priority**: Medium | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- PR diff file created
- Diff size output available

**Test Steps**:
1. Read diff size from step output
2. Parse as integer
3. Compare to threshold (50000 bytes)
4. Skip review if over threshold

**Data**: Diff file size

**Expected Result**:
- Diff size correctly calculated
- Review skips if oversized
- Condition in `if:` statement works correctly
- No errors if size is 0

---

### Test Case WF-PERP-004: API Request Constructed Safely
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- jq installed
- PR diff file exists
- API request JSON syntax defined

**Test Steps**:
1. Read diff content (max 10000 chars)
2. Construct JSON using jq: `jq -n --arg diff "$DIFF_TEXT" '{model: "sonar-pro", messages: [...]}'`
3. Verify JSON validity
4. Check no injection vulnerabilities
5. Verify request structure valid

**Data**: PR diff content

**Expected Result**:
- JSON request valid and parseable
- Diff content properly escaped
- No injection vulnerabilities
- Request matches API spec

---

### Test Case WF-PERP-005: API Response Validated
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Perplexity API accessible
- Valid API key
- Request successfully sent

**Test Steps**:
1. Send request to Perplexity API
2. Capture HTTP response code
3. Verify response code = 200
4. Parse JSON response
5. Extract review content
6. Verify content not null

**Data**: API response

**Expected Result**:
- HTTP 200 response
- Valid JSON response
- Review content extracted
- No "null" values
- Exit code 0

**Manual Verification Required**: Actual API call testing (can use mock)

---

### Test Case WF-PERP-006: PR Comment Posted
**Priority**: High | **Type**: Functional | **Automation**: Partial

**Preconditions**:
- Review generated
- PR context available
- gh CLI available
- GitHub token configured

**Test Steps**:
1. Get review content from API
2. Format comment with markdown
3. Post comment: `gh pr comment $PR_NUMBER --body "..."`
4. Verify comment created
5. Verify comment visible on PR

**Data**: Review content, PR number

**Expected Result**:
- Comment successfully posted
- Comment visible on PR
- Formatting correct
- Reviewers can see comment

**Manual Verification Required**: PR comment verification

---

## Category 12: Performance & Error Handling

### Test Case WF-PERF-001: CI Workflows Execute < 5 Minutes
**Priority**: High | **Type**: Performance | **Automation**: Yes

**Preconditions**:
- CI workflow triggers normally
- Timing captured from GitHub Actions
- SLA defined: 300 seconds

**Test Steps**:
1. Trigger CI workflow
2. Monitor execution time
3. Extract total duration from GitHub Actions UI
4. Compare against 300 second threshold
5. Log timing for trend analysis

**Data**: Workflow execution duration

**Expected Result**:
- Execution completes in < 300 seconds
- No timeout warnings
- All steps complete successfully
- Consistent timing (within 10% variance)

---

### Test Case WF-PERF-002: Deploy Workflows Execute < 10 Minutes
**Priority**: High | **Type**: Performance | **Automation**: Yes

**Preconditions**:
- Deploy workflow triggers
- Timing captured
- SLA defined: 600 seconds

**Test Steps**:
1. Trigger deploy workflow
2. Monitor execution
3. Extract total duration
4. Compare against 600 second threshold
5. Track individual step timings

**Data**: Deploy workflow execution

**Expected Result**:
- Completes in < 600 seconds
- Build: < 3 min
- Tests: < 2 min
- Deploy: < 5 min
- Notification: < 1 min

---

### Test Case WF-PERF-003: Lint Workflows Execute < 3 Minutes
**Priority**: High | **Type**: Performance | **Automation**: Yes

**Preconditions**:
- Lint workflow triggers
- SLA defined: 180 seconds

**Test Steps**:
1. Trigger lint workflow
2. Monitor execution
3. Extract duration
4. Compare against 180 second threshold

**Data**: Lint workflow execution

**Expected Result**:
- Completes in < 180 seconds
- Linting completes quickly
- No performance degradation
- Consistent timing

---

### Test Case WF-ERR-001: Failure Notifications Configured
**Priority**: High | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- Workflows have failure notification steps
- Slack webhook configured (optional)

**Test Steps**:
1. Identify failure notification steps
2. Verify `if: ${{ failure() }}` condition
3. Check notification action exists
4. Verify notification payload valid
5. Check webhook URL configured (if applicable)

**Data**: Failure notification steps

**Expected Result**:
- All critical workflows have failure notifications
- Conditions properly check for failure
- Notifications properly formatted
- Webhook URLs valid

---

### Test Case WF-ERR-002: continue-on-error Used Correctly
**Priority**: Medium | **Type**: Functional | **Automation**: Yes

**Preconditions**:
- All steps with `continue-on-error` identified
- Purpose of each documented

**Test Steps**:
1. Find all `continue-on-error: true` steps
2. Verify they're used for:
   - Optional integrations
   - Non-critical steps
   - Steps that have fallbacks
3. Verify they're NOT used for:
   - Critical build/test steps
   - Required integrations

**Data**: All steps with continue-on-error

**Expected Result**:
- Used appropriately for optional steps
- Not overused for critical steps
- Workflow behavior clear and documented
