{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# NOTEBOOK BOOTSTRAP (NO STATIC FILES)\n",
    "# Supabase-only via PostgREST\n",
    "# ==========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- papermill parameters (safe defaults) ----\n",
    "client_id = globals().get(\"client_id\", os.getenv(\"CLIENT_ID\"))\n",
    "run_id = globals().get(\"run_id\", os.getenv(\"RUN_ID\", datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "output_dir = globals().get(\"output_dir\", None)  # intentionally unused (no static files)\n",
    "\n",
    "def _require_env_any(keys_any: List[str]) -> str:\n",
    "    for k in keys_any:\n",
    "        v = os.getenv(k)\n",
    "        if v:\n",
    "            return v\n",
    "    raise RuntimeError(f\"Missing required env var. Provide one of: {keys_any}\")\n",
    "\n",
    "SUPABASE_URL = _require_env_any([\"SUPABASE_URL\"])\n",
    "SUPABASE_KEY = _require_env_any([\"SUPABASE_ANON_KEY\", \"NEXT_PUBLIC_SUPABASE_ANON_KEY\"])\n",
    "\n",
    "# ---- PostgREST client (no supabase-py dependency) ----\n",
    "import requests\n",
    "\n",
    "def _rest_select_all(\n",
    "    relation: str,\n",
    "    select: str = \"*\",\n",
    "    where: Optional[Dict[str, Any]] = None,\n",
    "    page_size: int = 5000,\n",
    "    max_rows: int = 250000,\n",
    ") -> pd.DataFrame:\n",
    "    base = SUPABASE_URL.rstrip(\"/\") + \"/rest/v1/\" + relation\n",
    "    headers = {\n",
    "        \"apikey\": SUPABASE_KEY,\n",
    "        \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\"select\": select, \"limit\": page_size, \"offset\": offset}\n",
    "        if where:\n",
    "            # PostgREST filter syntax: col=eq.value, etc.\n",
    "            for k, v in where.items():\n",
    "                params[k] = f\"eq.{v}\"\n",
    "\n",
    "        r = requests.get(base, headers=headers, params=params, timeout=60)\n",
    "        if r.status_code >= 300:\n",
    "            raise RuntimeError(f\"PostgREST error {r.status_code}: {r.text[:500]}\")\n",
    "\n",
    "        batch = r.json() or []\n",
    "        rows.extend(batch)\n",
    "\n",
    "        if len(batch) < page_size or len(rows) >= max_rows:\n",
    "            break\n",
    "\n",
    "        offset += page_size\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---- canonical schema normalization ----\n",
    "CANON = {\n",
    "    \"customer_id\": [\"customer_id\", \"client_id\", \"supplier_id\"],\n",
    "    \"disbursement_date\": [\"disbursement_date\", \"funded_date\", \"disbursed_at\"],\n",
    "    \"disbursement_amount\": [\"disbursement_amount\", \"funded_amount\", \"principal_amount\", \"amount\"],\n",
    "    \"outstanding_balance\": [\"outstanding_balance\", \"outstanding\", \"balance\", \"current_balance\"],\n",
    "    \"dpd\": [\"dpd\", \"days_past_due\"],\n",
    "    \"apr_annual\": [\"apr_annual\", \"apr\", \"annual_rate\", \"interest_rate\"],\n",
    "    \"term_days\": [\"term_days\", \"term\", \"term_in_days\"],\n",
    "    \"snapshot_id\": [\"snapshot_id\"],\n",
    "    \"snapshot_date\": [\"snapshot_date\", \"as_of_date\", \"business_date\", \"date\"],\n",
    "    \"interest_cash\": [\"interest_cash\", \"interest_amount\", \"interest_paid\"],\n",
    "    \"fees_cash\": [\"fees_cash\", \"fees\", \"fee_amount\"],\n",
    "    \"other_cash\": [\"other_cash\", \"other_amount\"],\n",
    "    \"rebates_cash\": [\"rebates_cash\", \"rebates\", \"rebate_amount\"],\n",
    "}\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    rename = {}\n",
    "    for canon, variants in CANON.items():\n",
    "        found = None\n",
    "        for v in variants:\n",
    "            k = v.lower()\n",
    "            if k in lower_map:\n",
    "                found = lower_map[k]\n",
    "                break\n",
    "        if found and found != canon:\n",
    "            rename[found] = canon\n",
    "    if rename:\n",
    "        df = df.rename(columns=rename)\n",
    "\n",
    "    if \"disbursement_date\" in df.columns:\n",
    "        df[\"disbursement_date\"] = pd.to_datetime(df[\"disbursement_date\"], errors=\"coerce\")\n",
    "    if \"snapshot_date\" in df.columns:\n",
    "        df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_date\"], errors=\"coerce\")\n",
    "    for c in [\"disbursement_amount\", \"outstanding_balance\", \"dpd\", \"apr_annual\", \"term_days\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# ---- Load loans from a public view (recommended) ----\n",
    "LOANS_VIEW = os.getenv(\"LOANS_VIEW\", \"v_loan_portfolio_daily\")  # expose this via PostgREST\n",
    "loans = normalize_columns(_rest_select_all(LOANS_VIEW, select=\"*\"))\n",
    "\n",
    "if loans.empty:\n",
    "    raise RuntimeError(f\"Loaded 0 rows from {LOANS_VIEW}. Check PostgREST exposure/RLS/view name.\")\n",
    "\n",
    "if not client_id:\n",
    "    client_id = str(loans[\"customer_id\"].dropna().astype(str).iloc[0])\n",
    "\n",
    "print(f\"[INFO] Loaded loans={len(loans):,} rows | client_id={client_id} | run_id={run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfa8de",
   "metadata": {},
   "source": [
    "# Production-Grade Supabase-Only Bootstrap\n",
    "This notebook is refactored to use **Supabase/PostgREST as the only data source and output target**. All static file dependencies, ipywidgets, and local exports are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9bca2",
   "metadata": {},
   "source": [
    "# Client Financial Analysis Notebook\n",
    "\n",
    "This notebook integrates all key financial formulas and reusable code to analyze any client by their ID. It is structured for business and risk analysis in B2B/ERP lending portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89816711",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import pandas, numpy, and any other libraries needed for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABACO NOTEBOOK BOOTSTRAP (Single Cell, CI/Papermill Safe)\n",
    "# - Restores built-ins if shadowed\n",
    "# - Loads parameters (client_id/run_id/output_dir)\n",
    "# - Defines canonical schema + normalize_columns()\n",
    "# - Loads loans from Supabase if credentials+package exist, else CSV fallback\n",
    "# - Fails ONLY if both Supabase and CSV are unavailable\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import builtins as _builtins\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Restore built-ins (safety)\n",
    "# ----------------------------\n",
    "for _name in (\"print\", \"len\", \"sum\", \"min\", \"max\", \"list\", \"dict\", \"set\", \"input\", \"any\", \"all\", \"sorted\", \"map\"):\n",
    "    if _name in globals() and globals()[_name] is not getattr(_builtins, _name, None):\n",
    "        globals()[_name] = getattr(_builtins, _name)\n",
    "del _name, _builtins\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Pandas display options\n",
    "# ----------------------------\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\" if abs(x) < 1 else f\"{x:,.2f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Papermill/CI parameters\n",
    "# ----------------------------\n",
    "# If papermill injects these, they already exist in globals().\n",
    "# Otherwise pull from env, else leave None (client_id can be chosen later from data).\n",
    "client_id = globals().get(\"client_id\", None) or os.getenv(\"CLIENT_ID\") or None\n",
    "run_id = globals().get(\"run_id\", None) or os.getenv(\"RUN_ID\") or datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = globals().get(\"output_dir\", None) or os.getenv(\"OUTPUT_DIR\") or \"outputs\"\n",
    "\n",
    "print(f\"[BOOTSTRAP] Parameters: client_id={client_id}, run_id={run_id}, output_dir={output_dir}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Project root resolution\n",
    "# ----------------------------\n",
    "def find_project_root(markers: Tuple[str, ...] = (\"pyproject.toml\", \"requirements.txt\", \".git\")) -> Path:\n",
    "    p = Path.cwd().resolve()\n",
    "    for _ in range(12):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\") or find_project_root()).resolve()\n",
    "print(f\"[BOOTSTRAP] PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Canonical schema mapping\n",
    "# ----------------------------\n",
    "CANON: Dict[str, List[str]] = {\n",
    "    \"customer_id\": [\"customer_id\", \"client_id\", \"supplier_id\"],\n",
    "    \"loan_id\": [\"loan_id\", \"id\"],\n",
    "    \"snapshot_date\": [\"snapshot_date\", \"as_of_date\", \"business_date\", \"date\"],\n",
    "    \"disbursement_date\": [\"disbursement_date\", \"disbursed_at\", \"funded_date\", \"disbursement_dt\"],\n",
    "    \"disbursement_amount\": [\"disbursement_amount\", \"loan_amount\", \"amount\", \"funded_amount\", \"principal_amount\"],\n",
    "    \"outstanding_balance\": [\"outstanding_balance\", \"outstanding\", \"balance\", \"current_balance\", \"principal_balance\"],\n",
    "    \"dpd\": [\"dpd\", \"DPD\", \"days_past_due\"],\n",
    "    \"apr_annual\": [\"apr_annual\", \"interest_rate\", \"apr\", \"APR\", \"annual_rate\"],\n",
    "    \"term_days\": [\"term_days\", \"term\", \"term_in_days\"],\n",
    "    \"interest_cash\": [\"interest_cash\", \"Interest\", \"interest_paid\", \"interest_amount\", \"interest_income\"],\n",
    "    \"fees_cash\": [\"fees_cash\", \"Fees\", \"fee_amount\", \"fees\"],\n",
    "    \"other_cash\": [\"other_cash\", \"Other\", \"other_amount\"],\n",
    "    \"rebates_cash\": [\"rebates_cash\", \"Rebates\", \"rebate_amount\"],\n",
    "    \"funding_cost\": [\"funding_cost\", \"cost_of_funds\", \"funding_expense\"],\n",
    "}\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    rename: Dict[str, str] = {}\n",
    "\n",
    "    for canon, variants in CANON.items():\n",
    "        found = None\n",
    "        for v in variants:\n",
    "            k = v.lower()\n",
    "            if k in lower_map:\n",
    "                found = lower_map[k]\n",
    "                break\n",
    "        if found and found != canon:\n",
    "            rename[found] = canon\n",
    "\n",
    "    if rename:\n",
    "        df = df.rename(columns=rename)\n",
    "\n",
    "    # Strong typing\n",
    "    if \"disbursement_date\" in df.columns:\n",
    "        df[\"disbursement_date\"] = pd.to_datetime(df[\"disbursement_date\"], errors=\"coerce\")\n",
    "\n",
    "    if \"snapshot_date\" in df.columns:\n",
    "        df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_date\"], errors=\"coerce\")\n",
    "\n",
    "    for c in (\"disbursement_amount\", \"outstanding_balance\", \"dpd\", \"apr_annual\", \"term_days\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"[BOOTSTRAP] normalize_columns() ready.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Safe numeric utilities\n",
    "# ----------------------------\n",
    "def safe_div(n: float, d: float) -> float:\n",
    "    try:\n",
    "        return float(n) / float(d) if d and d != 0 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def require_columns(df: pd.DataFrame, cols: List[str], ctx: str = \"\") -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns {missing}{(' in ' + ctx) if ctx else ''}\")\n",
    "\n",
    "def weighted_avg(values: pd.Series, weights: pd.Series) -> float:\n",
    "    v = pd.to_numeric(values, errors=\"coerce\")\n",
    "    w = pd.to_numeric(weights, errors=\"coerce\")\n",
    "    m = v.notna() & w.notna() & (w > 0)\n",
    "    return float(np.average(v[m], weights=w[m])) if m.any() else np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Supabase: only if credentials + package exist\n",
    "# ----------------------------\n",
    "def _supabase_credentials_present() -> bool:\n",
    "    if not os.getenv(\"SUPABASE_URL\"):\n",
    "        return False\n",
    "    # accept ANY key\n",
    "    key_candidates = (\n",
    "        \"SUPABASE_SERVICE_ROLE_KEY\",\n",
    "        \"SUPABASE_ANON_KEY\",\n",
    "        \"NEXT_PUBLIC_SUPABASE_ANON_KEY\",\n",
    "        \"SUPABASE_KEY\",\n",
    "    )\n",
    "    return any(os.getenv(k) for k in key_candidates)\n",
    "\n",
    "def _get_supabase_key() -> Optional[str]:\n",
    "    for k in (\"SUPABASE_SERVICE_ROLE_KEY\", \"SUPABASE_ANON_KEY\", \"NEXT_PUBLIC_SUPABASE_ANON_KEY\", \"SUPABASE_KEY\"):\n",
    "        v = os.getenv(k)\n",
    "        if v:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _try_import_supabase():\n",
    "    try:\n",
    "        from supabase import create_client  # noqa: F401\n",
    "        return create_client\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_CREATE_CLIENT = _try_import_supabase()\n",
    "\n",
    "def load_loans_from_supabase(\n",
    "    table_or_view_candidates: Optional[List[str]] = None,\n",
    "    page_size: int = 5000,\n",
    "    max_rows: int = 250000,\n",
    "    throttle_seconds: float = 0.05,\n",
    " ) -> pd.DataFrame:\n",
    "    if _CREATE_CLIENT is None:\n",
    "        raise ImportError(\"supabase package not installed.\")\n",
    "    if not _supabase_credentials_present():\n",
    "        raise EnvironmentError(\"Supabase credentials not present (SUPABASE_URL + any key).\")\n",
    "\n",
    "    if table_or_view_candidates is None:\n",
    "        table_or_view_candidates = [\n",
    "            \"v_loan_portfolio_daily\",\n",
    "            \"loan_portfolio_daily\",\n",
    "            \"curated.loan_portfolio_daily\",\n",
    "        ]\n",
    "\n",
    "    url = os.getenv(\"SUPABASE_URL\")\n",
    "    key = _get_supabase_key()\n",
    "    sb = _CREATE_CLIENT(url, key)\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "    for name in table_or_view_candidates:\n",
    "        try:\n",
    "            rows: List[Dict[str, Any]] = []\n",
    "            offset = 0\n",
    "            while True:\n",
    "                resp = (\n",
    "                    sb.table(name)\n",
    "                      .select(\"*\")\n",
    "                      .range(offset, offset + page_size - 1)\n",
    "                      .execute()\n",
    "                )\n",
    "                batch = resp.data or []\n",
    "                rows.extend(batch)\n",
    "\n",
    "                if len(batch) < page_size:\n",
    "                    break\n",
    "                offset += page_size\n",
    "                if len(rows) >= max_rows:\n",
    "                    break\n",
    "                if throttle_seconds:\n",
    "                    time.sleep(throttle_seconds)\n",
    "\n",
    "            df = pd.DataFrame(rows)\n",
    "            if df.empty:\n",
    "                raise ValueError(f\"Loaded 0 rows from '{name}'\")\n",
    "            return normalize_columns(df)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"All Supabase table/view candidates failed. Last error: {last_err}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CSV fallback (root-safe)\n",
    "# ----------------------------\n",
    "def load_loans_from_csv(path: str | Path) -> pd.DataFrame:\n",
    "    p = Path(path).expanduser()\n",
    "    if not p.is_absolute():\n",
    "        p = (PROJECT_ROOT / p).resolve()\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {p}\")\n",
    "    df = pd.read_csv(p, parse_dates=False)\n",
    "    return normalize_columns(df)\n",
    "\n",
    "# Default CSV locations to try (first existing wins)\n",
    "CSV_FALLBACK_CANDIDATES: List[Path] = [\n",
    "    Path(os.getenv(\"LOANS_CSV\", \"\")) if os.getenv(\"LOANS_CSV\") else Path(),\n",
    "    PROJECT_ROOT / \"data\" / \"loans.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"loans.parquet\",  # optional, if you later support parquet\n",
    "]\n",
    "\n",
    "def load_loans() -> pd.DataFrame:\n",
    "    supabase_err = None\n",
    "    csv_err = None\n",
    "\n",
    "    # 1) Supabase attempt if possible\n",
    "    if _supabase_credentials_present() and _CREATE_CLIENT is not None:\n",
    "        try:\n",
    "            df = load_loans_from_supabase()\n",
    "            print(f\"[DATA] Loaded {len(df):,} rows from Supabase.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            supabase_err = e\n",
    "            print(f\"[DATA] Supabase load failed (will fallback): {type(e).__name__}: {e}\")\n",
    "    else:\n",
    "        print(\"[DATA] Supabase not used (missing creds and/or package).\")\n",
    "\n",
    "    # 2) CSV fallback (first existing candidate)\n",
    "    for cand in CSV_FALLBACK_CANDIDATES:\n",
    "        if not cand or str(cand).strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            if cand.suffix.lower() == \".parquet\":\n",
    "                # Optional parquet support if needed later\n",
    "                if not cand.exists():\n",
    "                    continue\n",
    "                df = pd.read_parquet(cand)\n",
    "                df = normalize_columns(df)\n",
    "                print(f\"[DATA] Loaded {len(df):,} rows from Parquet: {cand}\")\n",
    "                return df\n",
    "            else:\n",
    "                df = load_loans_from_csv(cand)\n",
    "                print(f\"[DATA] Loaded {len(df):,} rows from CSV: {cand}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            csv_err = e\n",
    "            continue\n",
    "\n",
    "    # 3) Fail only if both unavailable\n",
    "    raise RuntimeError(\n",
    "        \"No loans data available from Supabase or CSV.\\n\"\n",
    "        f\"Supabase error: {supabase_err}\\n\"\n",
    "        f\"CSV error: {csv_err}\\n\"\n",
    "        f\"Tried CSV candidates: {[str(c) for c in CSV_FALLBACK_CANDIDATES if str(c).strip()]}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Load loans now (single source of truth)\n",
    "# ----------------------------\n",
    "loans = load_loans()\n",
    "require_columns(loans, [\"customer_id\", \"outstanding_balance\", \"dpd\"], \"loans\")\n",
    "print(f\"[BOOTSTRAP] loans ready. cols={len(loans.columns)}, clients={loans['customer_id'].nunique(dropna=True):,}\")\n",
    "display(loans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5018a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Parameters: client_id=None, run_id=None, output_dir=outputs/\n"
     ]
    }
   ],
   "source": [
    "# Papermill/CI parameter cell (production)\n",
    "# These are injected by the batch runner for each client\n",
    "\n",
    "try:\n",
    "    client_id\n",
    "except NameError:\n",
    "    client_id = None\n",
    "try:\n",
    "    run_id\n",
    "except NameError:\n",
    "    run_id = None\n",
    "try:\n",
    "    output_dir\n",
    "except NameError:\n",
    "    output_dir = \"outputs/\"\n",
    "\n",
    "print(f\"[INFO] Parameters: client_id={client_id}, run_id={run_id}, output_dir={output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a933d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT=/Users/jenineferderas/Documents/abaco-loans-analytics\n",
      "[WARN] Supabase load skipped/failed: supabase not installed\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV not found: /Users/jenineferderas/Documents/abaco-loans-analytics/data/loans.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# CSV fallback\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loans \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m loans.empty:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     loans = \u001b[43mload_loans_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloans.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loans)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows from CSV.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loans \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m loans.empty:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mload_loans_from_csv\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     30\u001b[39m     p = (PROJECT_ROOT / p).resolve()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p.exists():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCSV not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m normalize_columns(pd.read_csv(p, parse_dates=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: CSV not found: /Users/jenineferderas/Documents/abaco-loans-analytics/data/loans.csv"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# CANONICAL DATA LOADER (SUPABASE/CSV, PROJECT-ROOT SAFE)\n",
    "# ==========================\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_project_root(markers=(\"pyproject.toml\", \"requirements.txt\", \".git\")) -> Path:\n",
    "    p = Path.cwd().resolve()\n",
    "    for _ in range(10):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(f\"[INFO] PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "\n",
    "try:\n",
    "    from supabase import create_client\n",
    "except Exception:\n",
    "    create_client = None\n",
    "\n",
    "def load_loans_from_csv(path: str | Path) -> pd.DataFrame:\n",
    "    p = Path(path).expanduser()\n",
    "    if not p.is_absolute():\n",
    "        p = (PROJECT_ROOT / p).resolve()\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {p}\")\n",
    "    return normalize_columns(pd.read_csv(p, parse_dates=False))\n",
    "\n",
    "def load_loans_from_supabase(table=\"v_loan_portfolio_daily\", limit=200000) -> pd.DataFrame:\n",
    "    if create_client is None:\n",
    "        raise ImportError(\"supabase not installed\")\n",
    "    url = os.getenv(\"SUPABASE_URL\")\n",
    "    key = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\") or os.getenv(\"SUPABASE_ANON_KEY\") or os.getenv(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "    if not url or not key:\n",
    "        raise EnvironmentError(\"Missing SUPABASE_URL and/or key\")\n",
    "    sb = create_client(url, key)\n",
    "    resp = sb.table(table).select(\"*\").limit(limit).execute()\n",
    "    return normalize_columns(pd.DataFrame(resp.data or []))\n",
    "\n",
    "loans = None\n",
    "supabase_err = None\n",
    "\n",
    "# Try Supabase first\n",
    "try:\n",
    "    loans = load_loans_from_supabase()\n",
    "    print(f\"[INFO] Loaded {len(loans):,} rows from Supabase.\")\n",
    "except Exception as e:\n",
    "    supabase_err = e\n",
    "    print(f\"[WARN] Supabase load skipped/failed: {e}\")\n",
    "\n",
    "# CSV fallback\n",
    "if loans is None or loans.empty:\n",
    "    loans = load_loans_from_csv(PROJECT_ROOT / \"data\" / \"loans.csv\")\n",
    "    print(f\"[INFO] Loaded {len(loans):,} rows from CSV.\")\n",
    "\n",
    "if loans is None or loans.empty:\n",
    "    raise RuntimeError(f\"No loans data available. Supabase error={supabase_err}\")\n",
    "\n",
    "display(loans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6fde4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# PARAMETERIZED CLIENT SELECTION\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_client_widget\u001b[39m(df: pd.DataFrame, client_col: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mclient_id\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# PARAMETERIZED CLIENT SELECTION (OPTIONAL, SAFE FOR CI)\n",
    "# ==========================\n",
    "\n",
    "def select_client_widget(df: pd.DataFrame, client_col: str = \"customer_id\"):\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "    except Exception:\n",
    "        print(\"[WARN] ipywidgets not installed; widget selection disabled.\")\n",
    "        return None\n",
    "\n",
    "    clients = sorted(df[client_col].dropna().astype(str).unique())\n",
    "    dropdown = widgets.Dropdown(options=clients, description=\"Client:\")\n",
    "    display(dropdown)\n",
    "    return dropdown\n",
    "\n",
    "# Example usage:\n",
    "# client_selector = select_client_widget(loans)\n",
    "# selected_client = client_selector.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e344df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BALANCE-WEIGHTED PAR (PORTFOLIO AT RISK)\n",
    "# ==========================\n",
    "\n",
    "def calculate_par(df: pd.DataFrame, par_days: int = 30, principal_col: str = \"principal_balance\", days_past_due_col: str = \"days_past_due\") -> float:\n",
    "    if principal_col not in df.columns or days_past_due_col not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns: {principal_col}, {days_past_due_col}\")\n",
    "    total = df[principal_col].sum()\n",
    "    at_risk = df.loc[df[days_past_due_col] >= par_days, principal_col].sum()\n",
    "    return at_risk / total if total > 0 else 0.0\n",
    "\n",
    "# Example usage:\n",
    "# par_30 = calculate_par(loans, par_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# EFFECTIVE YIELD & NIMAL LOGIC (AUDIT-GRADE)\n",
    "# ==========================\n",
    "\n",
    "def calculate_effective_yield(df: pd.DataFrame, interest_col: str = \"interest_income\", principal_col: str = \"principal_balance\") -> float:\n",
    "    if interest_col not in df.columns or principal_col not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns: {interest_col}, {principal_col}\")\n",
    "    total_interest = df[interest_col].sum()\n",
    "    avg_principal = df[principal_col].mean()\n",
    "    return total_interest / avg_principal if avg_principal > 0 else 0.0\n",
    "\n",
    "def calculate_nimal(df: pd.DataFrame, interest_col: str = \"interest_income\", funding_cost_col: str = \"funding_cost\", principal_col: str = \"principal_balance\") -> float:\n",
    "    if any(col not in df.columns for col in [interest_col, funding_cost_col, principal_col]):\n",
    "        raise ValueError(f\"Missing required columns: {interest_col}, {funding_cost_col}, {principal_col}\")\n",
    "    net_income = df[interest_col].sum() - df[funding_cost_col].sum()\n",
    "    avg_principal = df[principal_col].mean()\n",
    "    return net_income / avg_principal if avg_principal > 0 else 0.0\n",
    "\n",
    "# Example usage:\n",
    "# eff_yield = calculate_effective_yield(loans)\n",
    "# nimal = calculate_nimal(loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8424b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# EEFF (AUDIT-GRADE) PARSER\n",
    "# ==========================\n",
    "\n",
    "def parse_eeff(df: pd.DataFrame, eeff_col: str = \"eeff_json\") -> pd.DataFrame:\n",
    "    if eeff_col not in df.columns:\n",
    "        raise ValueError(f\"Missing EEFF column: {eeff_col}\")\n",
    "    def parse_row(row):\n",
    "        try:\n",
    "            return json.loads(row[eeff_col]) if pd.notnull(row[eeff_col]) else {}\n",
    "        except Exception:\n",
    "            return {}\n",
    "    eeff_data = df.apply(parse_row, axis=1)\n",
    "    eeff_df = pd.json_normalize(eeff_data)\n",
    "    eeff_df.index = df.index\n",
    "    return eeff_df\n",
    "\n",
    "# Example usage:\n",
    "# eeff = parse_eeff(loans)\n",
    "# loans = pd.concat([loans, eeff], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# NOTEBOOK RESTORATION & CI/PRODUCTION SAFETY\n",
    "# ==========================\n",
    "\n",
    "def restore_notebook_state():\n",
    "    print(\"[INFO] Notebook state restored. All helpers, imports, and built-ins are present.\")\n",
    "    # Add any additional restoration logic here (e.g., re-imports, variable resets)\n",
    "\n",
    "# Call this at the end of the notebook or after cell execution if needed\n",
    "# restore_notebook_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# COLUMN NORMALIZATION (CANONICAL SCHEMA MAP)\n",
    "# ==========================================\n",
    "\n",
    "CANON = {\n",
    "    \"customer_id\": [\"customer_id\", \"client_id\", \"supplier_id\"],\n",
    "    \"loan_id\": [\"loan_id\", \"id\"],\n",
    "    \"snapshot_date\": [\"snapshot_date\", \"as_of_date\", \"business_date\", \"date\"],\n",
    "    \"disbursement_date\": [\"disbursement_date\", \"disbursed_at\", \"funded_date\", \"disbursement_dt\"],\n",
    "    \"disbursement_amount\": [\"disbursement_amount\", \"loan_amount\", \"amount\", \"funded_amount\"],\n",
    "    \"outstanding_balance\": [\"outstanding_balance\", \"outstanding\", \"balance\", \"current_balance\"],\n",
    "    \"dpd\": [\"dpd\", \"DPD\", \"days_past_due\"],\n",
    "    \"apr_annual\": [\"apr_annual\", \"interest_rate\", \"apr\", \"APR\", \"annual_rate\"],\n",
    "    \"term_days\": [\"term_days\", \"term\", \"term_in_days\"],\n",
    "    \"interest_cash\": [\"interest_cash\", \"Interest\", \"interest_paid\", \"interest_amount\"],\n",
    "    \"fees_cash\": [\"fees_cash\", \"Fees\", \"fee_amount\", \"fees\"],\n",
    "    \"other_cash\": [\"other_cash\", \"Other\", \"other_amount\"],\n",
    "    \"rebates_cash\": [\"rebates_cash\", \"Rebates\", \"rebate_amount\"],\n",
    "}\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    rename = {}\n",
    "    for canon, variants in CANON.items():\n",
    "        found = None\n",
    "        for v in variants:\n",
    "            k = v.lower()\n",
    "            if k in lower_map:\n",
    "                found = lower_map[k]\n",
    "                break\n",
    "        if found and found != canon:\n",
    "            rename[found] = canon\n",
    "    if rename:\n",
    "        df = df.rename(columns=rename)\n",
    "    # Strong typing\n",
    "    if \"disbursement_date\" in df.columns:\n",
    "        df[\"disbursement_date\"] = pd.to_datetime(df[\"disbursement_date\"], errors=\"coerce\")\n",
    "    if \"snapshot_date\" in df.columns:\n",
    "        df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_date\"], errors=\"coerce\").dt.date\n",
    "    for c in (\"disbursement_amount\", \"outstanding_balance\", \"dpd\", \"apr_annual\", \"term_days\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "print(\"[BOOTSTRAP] normalize_columns() loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# NOTEBOOK BOOTSTRAP (ABACO)\n",
    "# =========================\n",
    "\n",
    "# 1) Restore Python built-ins if shadowed in notebook scope\n",
    "# (common causes: variables named `len`, `list`, `print`, `sum`, `input`, etc.)\n",
    "import builtins as _builtins\n",
    "for _name in (\"print\", \"len\", \"sum\", \"min\", \"max\", \"list\", \"dict\", \"set\", \"input\", \"any\", \"all\", \"sorted\", \"map\"):\n",
    "    if _name in globals() and globals()[_name] is not getattr(_builtins, _name, None):\n",
    "        globals()[_name] = getattr(_builtins, _name)\n",
    "del _name, _builtins\n",
    "\n",
    "# 2) Core imports (avoid \"NameError: Path is not defined\", etc.)\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 3) Pandas settings (optional)\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\" if abs(x) < 1 else f\"{x:,.2f}\")\n",
    "\n",
    "# 4) Optional dependencies: supabase client\n",
    "# If not installed, the notebook can still run with CSV fallback.\n",
    "try:\n",
    "    from supabase import create_client\n",
    "except Exception:\n",
    "    create_client = None\n",
    "\n",
    "print(\"[BOOTSTRAP] Ready. Built-ins restored, imports loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68983557",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Missing required env vars: ['SUPABASE_URL', 'SUPABASE_ANON_KEY']. Set them before running the notebook.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m missing = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m REQUIRED_ENV \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.getenv(k)]\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required env vars: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Set them before running the notebook.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Missing required env vars: ['SUPABASE_URL', 'SUPABASE_ANON_KEY']. Set them before running the notebook."
     ]
    }
   ],
   "source": [
    "# --- Config + Environment Validation (must run first) ---\n",
    "import os\n",
    "REQUIRED_ENV = [\"SUPABASE_URL\", \"SUPABASE_ANON_KEY\"]\n",
    "missing = [k for k in REQUIRED_ENV if not os.getenv(k)]\n",
    "if missing:\n",
    "    raise EnvironmentError(f\"Missing required env vars: {missing}. Set them before running the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae18d20",
   "metadata": {},
   "source": [
    "## 2. Define Financial Analysis Formulas\n",
    "Implement Python functions for the financial formulas used in client analysis (APR, yield, DPD, rotation, ticket, recurrence, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Financial Formulas for Client Analysis ---\n",
    "import builtins\n",
    "def weighted_average(series, weights):\n",
    "    return np.average(series, weights=weights) if builtins.len(series) > 0 else np.nan\n",
    "\n",
    "def apr_annual_nom(apr_m):\n",
    "    return apr_m * 12\n",
    "\n",
    "def apr_annual_eff(apr_m):\n",
    "    return (1 + apr_m) ** 12 - 1\n",
    "\n",
    "def yield_per_term(apr_m, term):\n",
    "    return (1 + apr_m) ** (term / 30) - 1\n",
    "\n",
    "def rotation(avg_term):\n",
    "    return 360 / avg_term if avg_term > 0 else np.nan\n",
    "\n",
    "def line_up(disb_first, disb_last):\n",
    "    return disb_last / disb_first - 1 if disb_first > 0 else np.nan\n",
    "\n",
    "def rate_down(apr_first, apr_last):\n",
    "    return apr_last - apr_first\n",
    "\n",
    "def tenor_up(term_first, term_last):\n",
    "    return term_last - term_first\n",
    "\n",
    "def dpd_proxy(days_realized, term):\n",
    "    return days_realized - term\n",
    "\n",
    "def percent_back_to_back(days_between, term):\n",
    "    return np.mean(days_between <= (term + 15)) if builtins.len(days_between) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1245716",
   "metadata": {},
   "source": [
    "## 3. Load Client Data\n",
    "Load the client dataset from a CSV file or database into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Supabase load failed: No module named 'supabase'. Trying CSV fallback...\n",
      "[ERROR] Could not load loans from Supabase or CSV: CSV not found: /Users/jenineferderas/Documents/abaco-loans-analytics/notebooks/data/loans.csv\n",
      "[ERROR] No loans data available. Please check your data source or configuration.\n"
     ]
    }
   ],
   "source": [
    "# --- Robust Supabase loader with paging and schema fallback ---\n",
    "from supabase import create_client\n",
    "import pandas as pd\n",
    "\n",
    "def load_loans_from_supabase(table_or_view_candidates=None, page_size: int = 5000, max_rows: int = 250000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust Supabase loader with paging. Works only for resources exposed via PostgREST.\n",
    "    If curated schema isn't exposed, create a public view and use that name.\n",
    "    \"\"\"\n",
    "    if table_or_view_candidates is None:\n",
    "        table_or_view_candidates = [\n",
    "            \"v_loan_portfolio_daily\",      # recommended public view name\n",
    "            \"loan_portfolio_daily\",        # alternative\n",
    "            \"curated.loan_portfolio_daily\" # will fail if schema not exposed\n",
    "        ]\n",
    "    url = os.environ[\"SUPABASE_URL\"]\n",
    "    key = os.environ[\"SUPABASE_ANON_KEY\"]\n",
    "    sb = create_client(url, key)\n",
    "    last_err = None\n",
    "    for name in table_or_view_candidates:\n",
    "        rows = []\n",
    "        offset = 0\n",
    "        try:\n",
    "            while True:\n",
    "                resp = (\n",
    "                    sb.table(name)\n",
    "                      .select(\"*\")\n",
    "                      .range(offset, offset + page_size - 1)\n",
    "                      .execute()\n",
    "                )\n",
    "                batch = resp.data or []\n",
    "                rows.extend(batch)\n",
    "                if len(batch) < page_size:\n",
    "                    break\n",
    "                offset += page_size\n",
    "                if len(rows) >= max_rows:\n",
    "                    break\n",
    "                time.sleep(0.05)  # gentle throttling\n",
    "            df = pd.DataFrame(rows)\n",
    "            if df.empty:\n",
    "                raise ValueError(f\"Loaded 0 rows from '{name}'\")\n",
    "            return normalize_columns(df)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"All Supabase table/view candidates failed. Last error: {last_err}\")\n",
    "\n",
    "# --- Data loading: Supabase default, CSV fallback ---\n",
    "def load_loans_from_csv(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {p.resolve()}\")\n",
    "    df = pd.read_csv(p, parse_dates=False)\n",
    "    return normalize_columns(df)\n",
    "\n",
    "try:\n",
    "    loans = load_loans_from_supabase()\n",
    "    print(f\"[INFO] Loaded {len(loans)} loans from Supabase.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Supabase load failed: {e}. Trying CSV fallback...\")\n",
    "    try:\n",
    "        loans = load_loans_from_csv(\"data/loans.csv\")\n",
    "        print(f\"[INFO] Loaded {len(loans)} loans from CSV.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[ERROR] Could not load loans from Supabase or CSV: {e2}\")\n",
    "        loans = None\n",
    "\n",
    "if loans is not None:\n",
    "    display(loans.head())\n",
    "else:\n",
    "    print(\"[ERROR] No loans data available. Please check your data source or configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb88230",
   "metadata": {},
   "source": [
    "## 4. Input Client ID for Analysis\n",
    "Prompt the user to input a client ID or select one from the dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] The loans DataFrame is not loaded. Please run the data loading cell above.\n"
     ]
    }
   ],
   "source": [
    "# --- Parameterized client selection (supports papermill/CLI) ---\n",
    "import os\n",
    "\n",
    "if loans is None or loans.empty:\n",
    "    raise ValueError(\"Loans dataset is empty.\")\n",
    "\n",
    "client_ids = loans[\"customer_id\"].dropna().astype(str).unique()\n",
    "print(f\"[INFO] Total unique clients: {len(client_ids)}\")\n",
    "print(f\"[INFO] Sample client IDs: {client_ids[:25]}\")\n",
    "\n",
    "client_id = None\n",
    "# Priority: injected parameter (e.g., via papermill or os.environ)\n",
    "if 'client_id' in globals() and client_id is not None:\n",
    "    pass  # already set\n",
    "elif os.environ.get('CLIENT_ID'):\n",
    "    client_id = os.environ['CLIENT_ID']\n",
    "else:\n",
    "    try:\n",
    "        client_id = input(\"Enter client ID: \").strip()\n",
    "        if client_id not in set(client_ids):\n",
    "            print(\"[WARNING] Client not found; using first sample.\")\n",
    "            client_id = client_ids[0]\n",
    "    except Exception:\n",
    "        print(\"[WARNING] input() not available; using first sample.\")\n",
    "        client_id = client_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452ebc8",
   "metadata": {},
   "source": [
    "## 5. Apply Formulas to Selected Client\n",
    "Filter the DataFrame for the selected client ID and apply the defined formulas to compute relevant metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1862cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] The loans DataFrame is not loaded. Please run the data loading cell above.\n"
     ]
    }
   ],
   "source": [
    "# --- Client metrics with portfolio benchmarks and APR guard ---\n",
    "def compute_client_metrics(loans: pd.DataFrame, client_id: str):\n",
    "    require_columns(loans, [\"customer_id\", \"disbursement_date\", \"disbursement_amount\", \"outstanding_balance\", \"dpd\"], \"loans\")\n",
    "    df = loans.copy()\n",
    "    df = df[df[\"customer_id\"].notna()]\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    client = df[df[\"customer_id\"] == str(client_id)].copy()\n",
    "    if client.empty:\n",
    "        raise ValueError(f\"No loans found for client_id={client_id}\")\n",
    "    total_outstanding = df[\"outstanding_balance\"].sum(skipna=True)\n",
    "    client_outstanding = client[\"outstanding_balance\"].sum(skipna=True)\n",
    "    par30_bal = client.loc[client[\"dpd\"] >= 30, \"outstanding_balance\"].sum(skipna=True)\n",
    "    par60_bal = client.loc[client[\"dpd\"] >= 60, \"outstanding_balance\"].sum(skipna=True)\n",
    "    par90_bal = client.loc[client[\"dpd\"] >= 90, \"outstanding_balance\"].sum(skipna=True)\n",
    "    avg_term = weighted_avg(client[\"term_days\"], client[\"disbursement_amount\"]) if \"term_days\" in client.columns else np.nan\n",
    "    avg_apr_annual = weighted_avg(client[\"apr_annual\"], client[\"disbursement_amount\"]) if \"apr_annual\" in client.columns else np.nan\n",
    "    client = client.sort_values(\"disbursement_date\")\n",
    "    client[\"days_between\"] = client[\"disbursement_date\"].diff().dt.days\n",
    "    b2b = np.nan\n",
    "    if \"term_days\" in client.columns and client[\"days_between\"].notna().any():\n",
    "        term_ref = float(pd.to_numeric(client[\"term_days\"], errors=\"coerce\").mean())\n",
    "        b2b = float((client[\"days_between\"] <= (term_ref + 15)).mean())\n",
    "    results = {\n",
    "        \"Total Disbursed\": float(client[\"disbursement_amount\"].sum(skipna=True)),\n",
    "        \"Current Outstanding\": float(client_outstanding),\n",
    "        \"Share of Portfolio Outstanding\": safe_div(client_outstanding, total_outstanding),\n",
    "        \"Max DPD\": float(client[\"dpd\"].max(skipna=True)),\n",
    "        \"Mean DPD\": float(client[\"dpd\"].mean(skipna=True)),\n",
    "        \"% Exposure DPD>=30\": safe_div(par30_bal, client_outstanding),\n",
    "        \"% Exposure DPD>=60\": safe_div(par60_bal, client_outstanding),\n",
    "        \"% Exposure DPD>=90\": safe_div(par90_bal, client_outstanding),\n",
    "        \"PAR30 (Balance)\": safe_div(par30_bal, client_outstanding),\n",
    "        \"PAR60 (Balance)\": safe_div(par60_bal, client_outstanding),\n",
    "        \"PAR90 (Balance)\": safe_div(par90_bal, client_outstanding),\n",
    "        \"Avg Ticket\": float(client[\"disbursement_amount\"].mean(skipna=True)),\n",
    "        \"Median Days Between\": float(client[\"days_between\"].median(skipna=True)) if client[\"days_between\"].notna().any() else np.nan,\n",
    "        \"P75 Days Between\": float(client[\"days_between\"].quantile(0.75)) if client[\"days_between\"].notna().any() else np.nan,\n",
    "        \"% Back-to-Back\": b2b,\n",
    "        \"Avg Term (days)\": float(avg_term) if not np.isnan(avg_term) else np.nan,\n",
    "        \"Rotation (annualized)\": safe_div(360.0, avg_term) if not np.isnan(avg_term) else np.nan,\n",
    "        \"APR Annual (weighted)\": float(avg_apr_annual) if not np.isnan(avg_apr_annual) else np.nan,\n",
    "    }\n",
    "    if len(client) > 1:\n",
    "        first, last = client.iloc[0], client.iloc[-1]\n",
    "        results[\"Line Up (firstlast)\"] = safe_div(last[\"disbursement_amount\"], first[\"disbursement_amount\"]) - 1\n",
    "        if \"apr_annual\" in client.columns:\n",
    "            results[\"Rate Delta (bps)\"] = (float(last.get(\"apr_annual\", np.nan)) - float(first.get(\"apr_annual\", np.nan))) * 10000\n",
    "        if \"term_days\" in client.columns:\n",
    "            results[\"Tenor Delta (days)\"] = float(last.get(\"term_days\", np.nan)) - float(first.get(\"term_days\", np.nan))\n",
    "    else:\n",
    "        results[\"Line Up (firstlast)\"] = np.nan\n",
    "        results[\"Rate Delta (bps)\"] = np.nan\n",
    "        results[\"Tenor Delta (days)\"] = np.nan\n",
    "    results_df = pd.DataFrame(results, index=[str(client_id)])\n",
    "    # --- Portfolio benchmarks ---\n",
    "    def portfolio_benchmarks(loans: pd.DataFrame) -> dict:\n",
    "        require_columns(loans, [\"outstanding_balance\", \"dpd\"], \"loans\")\n",
    "        total = loans[\"outstanding_balance\"].sum(skipna=True)\n",
    "        par30 = loans.loc[loans[\"dpd\"] >= 30, \"outstanding_balance\"].sum(skipna=True)\n",
    "        par90 = loans.loc[loans[\"dpd\"] >= 90, \"outstanding_balance\"].sum(skipna=True)\n",
    "        return {\n",
    "            \"Portfolio Outstanding\": float(total),\n",
    "            \"Portfolio PAR30\": safe_div(par30, total),\n",
    "            \"Portfolio PAR90\": safe_div(par90, total),\n",
    "        }\n",
    "    bench = portfolio_benchmarks(loans)\n",
    "    for k, v in bench.items():\n",
    "        results_df.loc[str(client_id), k] = v\n",
    "    # --- APR guard ---\n",
    "    if \"apr_annual\" in loans.columns and loans[\"apr_annual\"].dropna().between(0, 3).mean() < 0.8:\n",
    "        print(\"[WARNING] apr_annual has values outside [0,3]. Verify if rates are in % or decimals.\")\n",
    "    return results_df, client\n",
    "\n",
    "# --- Usage: compute metrics ---\n",
    "results_df, client_loans = compute_client_metrics(loans, client_id)\n",
    "display(results_df)\n",
    "display(client_loans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Output classification: Watchlist / Run-off / Top Clients ---\n",
    "def classify_client(results: pd.Series) -> dict:\n",
    "    par90 = results.get(\"PAR90 (Balance)\", np.nan)\n",
    "    share = results.get(\"Share of Portfolio Outstanding\", np.nan)\n",
    "    dpd_max = results.get(\"Max DPD\", np.nan)\n",
    "    if pd.notna(par90) and par90 >= 0.10:\n",
    "        tier = \"RUN_OFF\"\n",
    "    elif pd.notna(dpd_max) and dpd_max >= 60:\n",
    "        tier = \"WATCHLIST\"\n",
    "    elif pd.notna(share) and share >= 0.05:\n",
    "        tier = \"TOP_CLIENT\"\n",
    "    else:\n",
    "        tier = \"STANDARD\"\n",
    "    return {\"tier\": tier}\n",
    "\n",
    "cls = classify_client(results_df.loc[str(client_id)])\n",
    "results_df.loc[str(client_id), \"Client Tier\"] = cls[\"tier\"]\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffddfea8",
   "metadata": {},
   "source": [
    "## 6. Display Analysis Results\n",
    "Present the computed metrics and analysis results for the selected client in a clear, tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018505d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client_loans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m         plt.show()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- Usage ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m plot_client(\u001b[43mclient_loans\u001b[49m, client_id)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Export results to CSV and Excel\u001b[39;00m\n\u001b[32m     24\u001b[39m results_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mclient_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_analysis.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client_loans' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Matplotlib-only visuals for client analysis (robust, CI-friendly) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_client(client_loans: pd.DataFrame, client_id: str):\n",
    "    df = client_loans.sort_values(\"disbursement_date\").copy()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.hist(df[\"dpd\"].dropna(), bins=20)\n",
    "    plt.title(f\"DPD Distribution  {client_id}\")\n",
    "    plt.xlabel(\"DPD (days)\")\n",
    "    plt.ylabel(\"Loans\")\n",
    "    plt.show()\n",
    "    if \"apr_annual\" in df.columns:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(df[\"disbursement_date\"], df[\"apr_annual\"], marker=\"o\")\n",
    "        plt.title(f\"APR Annual Trend  {client_id}\")\n",
    "        plt.ylabel(\"APR annual\")\n",
    "        plt.xlabel(\"Disbursement date\")\n",
    "        plt.show()\n",
    "\n",
    "# --- Usage ---\n",
    "plot_client(client_loans, client_id)\n",
    "\n",
    "# --- Minimal, safe exports ---\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "out_dir = Path(\"outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_df.to_csv(out_dir / f\"client_{client_id}_analysis_{ts}.csv\", index=True)\n",
    "results_df.to_excel(out_dir / f\"client_{client_id}_analysis_{ts}.xlsx\", index=True)\n",
    "print(f\"[INFO] Exported to {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754fd2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client_loans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Complete the metrics ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Effective yield (cash, per loan)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mInterest\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient_loans\u001b[49m.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mFees\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m client_loans.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mOther\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m client_loans.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mRebates\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m client_loans.columns:\n\u001b[32m      4\u001b[39m     client_loans[\u001b[33m'\u001b[39m\u001b[33mYield_term\u001b[39m\u001b[33m'\u001b[39m] = (client_loans[\u001b[33m'\u001b[39m\u001b[33mInterest\u001b[39m\u001b[33m'\u001b[39m] + client_loans[\u001b[33m'\u001b[39m\u001b[33mFees\u001b[39m\u001b[33m'\u001b[39m] + client_loans[\u001b[33m'\u001b[39m\u001b[33mOther\u001b[39m\u001b[33m'\u001b[39m] - client_loans[\u001b[33m'\u001b[39m\u001b[33mRebates\u001b[39m\u001b[33m'\u001b[39m]) / client_loans[\u001b[33m'\u001b[39m\u001b[33mdisbursement_amount\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mEffective Yield (mean)\u001b[39m\u001b[33m'\u001b[39m] = client_loans[\u001b[33m'\u001b[39m\u001b[33mYield_term\u001b[39m\u001b[33m'\u001b[39m].mean()\n",
      "\u001b[31mNameError\u001b[39m: name 'client_loans' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Improved Effective Yield (cash-based) + Annualization ---\n",
    "import pandas as pd\n",
    "\n",
    "def add_effective_yield(client_loans: pd.DataFrame, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = client_loans.copy()\n",
    "    required = [\"disbursement_amount\", \"interest_cash\", \"fees_cash\", \"other_cash\", \"rebates_cash\"]\n",
    "    if not all(c in df.columns for c in required):\n",
    "        results_df.loc[:, \"Effective Yield (term, mean)\"] = np.nan\n",
    "        results_df.loc[:, \"Effective Yield (term, weighted)\"] = np.nan\n",
    "        results_df.loc[:, \"Effective Yield (annualized, weighted)\"] = np.nan\n",
    "        return results_df\n",
    "    disb = pd.to_numeric(df[\"disbursement_amount\"], errors=\"coerce\")\n",
    "    disb = disb.where(disb > 0, np.nan)\n",
    "    cash = (\n",
    "        pd.to_numeric(df[\"interest_cash\"], errors=\"coerce\").fillna(0)\n",
    "        + pd.to_numeric(df[\"fees_cash\"], errors=\"coerce\").fillna(0)\n",
    "        + pd.to_numeric(df[\"other_cash\"], errors=\"coerce\").fillna(0)\n",
    "        - pd.to_numeric(df[\"rebates_cash\"], errors=\"coerce\").fillna(0)\n",
    "    )\n",
    "    df[\"yield_term\"] = cash / disb\n",
    "    results_df.loc[:, \"Effective Yield (term, mean)\"] = float(df[\"yield_term\"].mean(skipna=True))\n",
    "    results_df.loc[:, \"Effective Yield (term, weighted)\"] = weighted_avg(df[\"yield_term\"], df[\"disbursement_amount\"])\n",
    "    # Optional: annualize if term_days is available (360-day convention)\n",
    "    if \"term_days\" in df.columns:\n",
    "        term = pd.to_numeric(df[\"term_days\"], errors=\"coerce\")\n",
    "        m = df[\"yield_term\"].notna() & term.notna() & (term > 0)\n",
    "        if m.any():\n",
    "            annualized = (1 + df.loc[m, \"yield_term\"]) ** (360.0 / term.loc[m]) - 1\n",
    "            results_df.loc[:, \"Effective Yield (annualized, weighted)\"] = float(\n",
    "                np.average(annualized, weights=pd.to_numeric(df.loc[m, \"disbursement_amount\"], errors=\"coerce\"))\n",
    "            )\n",
    "        else:\n",
    "            results_df.loc[:, \"Effective Yield (annualized, weighted)\"] = np.nan\n",
    "    else:\n",
    "        results_df.loc[:, \"Effective Yield (annualized, weighted)\"] = np.nan\n",
    "    return results_df\n",
    "\n",
    "# --- Robust EEFF Parsing + Deterministic Row Picking ---\n",
    "_NUM_CLEAN = re.compile(r\"[^\\d\\.\\-\\(\\)]+\")\n",
    "\n",
    "def parse_numeric(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return None\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
    "    s = s[1:-1] if neg else s\n",
    "    s = s.replace(\",\", \"\")\n",
    "    s = _NUM_CLEAN.sub(\"\", s)\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return -v if neg else v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_best_row(df: pd.DataFrame, patterns_ranked):\n",
    "    for pat in patterns_ranked:\n",
    "        rx = re.compile(pat, re.IGNORECASE)\n",
    "        for i, row in df.iterrows():\n",
    "            if any(rx.search(str(cell)) for cell in row.values if pd.notna(cell)):\n",
    "                return i\n",
    "    return None\n",
    "\n",
    "def extract_row_value(df: pd.DataFrame, row_idx: int, prefer_col: int | None = None):\n",
    "    row = df.iloc[row_idx]\n",
    "    if prefer_col is not None and 0 <= prefer_col < df.shape[1]:\n",
    "        v = parse_numeric(row.iloc[prefer_col])\n",
    "        if v is not None:\n",
    "            return v\n",
    "    candidates = []\n",
    "    for v in row.values:\n",
    "        n = parse_numeric(v)\n",
    "        if n is not None:\n",
    "            candidates.append(n)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "def compute_nimal_from_eeff(excel_path: str, sheet: str = \"PYG\", prefer_col: int | None = None) -> dict:\n",
    "    pyg = pd.read_excel(excel_path, sheet_name=sheet, header=None)\n",
    "    income_row = find_best_row(\n",
    "        pyg,\n",
    "        patterns_ranked=[\n",
    "            r\"\\bingresos?\\s+financier\",   # highest priority\n",
    "            r\"\\bingresos?\\b\",             # fallback\n",
    "        ],\n",
    "    )\n",
    "    cost_row = find_best_row(\n",
    "        pyg,\n",
    "        patterns_ranked=[\n",
    "            r\"\\bcosto\\s+de\\s+fond\",       # funding cost preferred\n",
    "            r\"\\bcosto\\s+direct\",          # fallback\n",
    "            r\"\\bcosto\\b|\\bcost\\b\",\n",
    "        ],\n",
    "    )\n",
    "    loss_row = find_best_row(\n",
    "        pyg,\n",
    "        patterns_ranked=[\n",
    "            r\"\\bp[e]rdid(as)?\\s+credit\", # credit losses preferred\n",
    "            r\"\\bcastig\",                  # charge-offs\n",
    "            r\"\\bp[e]rdid|loss\",\n",
    "        ],\n",
    "    )\n",
    "    if income_row is None or cost_row is None or loss_row is None:\n",
    "        return {\n",
    "            \"error\": \"Could not locate income/cost/loss rows reliably in EEFF.\",\n",
    "            \"income_row\": income_row,\n",
    "            \"cost_row\": cost_row,\n",
    "            \"loss_row\": loss_row,\n",
    "        }\n",
    "    income = extract_row_value(pyg, income_row, prefer_col=prefer_col)\n",
    "    cost = extract_row_value(pyg, cost_row, prefer_col=prefer_col)\n",
    "    loss = extract_row_value(pyg, loss_row, prefer_col=prefer_col)\n",
    "    if income is None or cost is None or loss is None:\n",
    "        return {\n",
    "            \"error\": \"Matched rows but could not parse numeric values (check EEFF formatting).\",\n",
    "            \"income\": income,\n",
    "            \"direct_cost\": cost,\n",
    "            \"losses\": loss,\n",
    "            \"income_row\": income_row,\n",
    "            \"cost_row\": cost_row,\n",
    "            \"loss_row\": loss_row,\n",
    "        }\n",
    "    nimal_cost = safe_div(income - loss, cost)\n",
    "    return {\n",
    "        \"income\": float(income),\n",
    "        \"direct_cost\": float(cost),\n",
    "        \"losses\": float(loss),\n",
    "        \"nimal_on_cost\": float(nimal_cost) if nimal_cost == nimal_cost else None,\n",
    "        \"income_row\": int(income_row),\n",
    "        \"cost_row\": int(cost_row),\n",
    "        \"loss_row\": int(loss_row),\n",
    "        \"prefer_col\": prefer_col,\n",
    "        \"notes\": \"NIMAL(on cost) = (income - losses) / direct_cost\",\n",
    "    }\n",
    "\n",
    "# --- AUM From Loan Tape as a True Average (Windowed) ---\n",
    "from datetime import datetime\n",
    "\n",
    "def avg_aum_from_loans(\n",
    "    loans: pd.DataFrame,\n",
    "    date_col: str = \"snapshot_date\",\n",
    "    window_days: int = 30,\n",
    "    as_of: str | None = None\n",
    ") -> float:\n",
    "    if \"outstanding_balance\" not in loans.columns:\n",
    "        raise ValueError(\"Missing outstanding_balance in loans\")\n",
    "    if date_col not in loans.columns:\n",
    "        return float(loans[\"outstanding_balance\"].sum(skipna=True))\n",
    "    df = loans[[date_col, \"outstanding_balance\"]].copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df[\"outstanding_balance\"] = pd.to_numeric(df[\"outstanding_balance\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[date_col, \"outstanding_balance\"])\n",
    "    if df.empty:\n",
    "        return float(loans[\"outstanding_balance\"].sum(skipna=True))\n",
    "    as_of_dt = pd.to_datetime(as_of) if as_of else df[date_col].max()\n",
    "    start_dt = as_of_dt - pd.Timedelta(days=window_days - 1)\n",
    "    w = df[(df[date_col] >= start_dt) & (df[date_col] <= as_of_dt)]\n",
    "    if w.empty:\n",
    "        return float(loans[\"outstanding_balance\"].sum(skipna=True))\n",
    "    daily = w.groupby(date_col)[\"outstanding_balance\"].sum()\n",
    "    return float(daily.mean())\n",
    "\n",
    "# --- Usage Block (Upgraded) ---\n",
    "results_df = add_effective_yield(client_loans, results_df)\n",
    "display(results_df)\n",
    "\n",
    "eeff = compute_nimal_from_eeff(\n",
    "    \"../data/EEFF ABACO CONSOLIDADO - ESTADO DE RESULTADOS - SEP-25.xlsx\",\n",
    "    sheet=\"PYG\",\n",
    "    prefer_col=None  # set an explicit column index once you identify the latest period column\n",
    ")\n",
    "\n",
    "if \"error\" not in eeff:\n",
    "    aum = avg_aum_from_loans(loans, date_col=\"snapshot_date\", window_days=30)\n",
    "    eeff[\"avg_aum_used\"] = aum\n",
    "    eeff[\"nimal_on_aum\"] = safe_div(eeff[\"income\"] - eeff[\"losses\"], aum)\n",
    "\n",
    "print(eeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c55713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Restore Python built-ins for notebook robustness ---\n",
    "all = builtins.all\n",
    "any = builtins.any\n",
    "isinstance = builtins.isinstance\n",
    "Exception = builtins.Exception\n",
    "ValueError = builtins.ValueError\n",
    "dict = builtins.dict\n",
    "print = builtins.print\n",
    "input = builtins.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38684dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Production-grade risk alerts (robust, structured) ---\n",
    "def anchor_concentration_alert(invoices: pd.DataFrame, client_id: str, threshold: float = 0.4) -> dict:\n",
    "    require_columns(invoices, [\"customer_id\", \"anchor_id\", \"invoice_amount\"], \"invoices\")\n",
    "    df = invoices.copy()\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    sub = df[df[\"customer_id\"] == str(client_id)]\n",
    "    if sub.empty:\n",
    "        return {\"alert\": False, \"reason\": \"no_invoices\"}\n",
    "    total = sub[\"invoice_amount\"].sum()\n",
    "    if total <= 0:\n",
    "        return {\"alert\": False, \"reason\": \"zero_total_amount\"}\n",
    "    shares = sub.groupby(\"anchor_id\")[\"invoice_amount\"].sum() / total\n",
    "    anchor = shares.idxmax()\n",
    "    share = float(shares.max())\n",
    "    return {\"alert\": share > threshold, \"anchor_id\": anchor, \"share\": share, \"threshold\": threshold}\n",
    "\n",
    "def credit_line_usage_alert(credit_used, credit_limit, threshold=0.9):\n",
    "    usage = safe_div(credit_used, credit_limit)\n",
    "    return {\"alert\": usage > threshold, \"usage\": usage, \"threshold\": threshold}\n",
    "\n",
    "def extended_term_alert(loans: pd.DataFrame, client_id: str, threshold=90):\n",
    "    require_columns(loans, [\"customer_id\", \"term_days\"], \"loans\")\n",
    "    df = loans.copy()\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    client_loans = df[df[\"customer_id\"] == str(client_id)]\n",
    "    long_terms = client_loans[client_loans[\"term_days\"] > threshold]\n",
    "    return {\"alert\": not long_terms.empty, \"long_terms\": long_terms}\n",
    "\n",
    "def apr_gap_alert(loans: pd.DataFrame, client_id: str, bps_gap=0.05):\n",
    "    require_columns(loans, [\"customer_id\", \"apr_annual\"], \"loans\")\n",
    "    df = loans.copy()\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    client_loans = df[df[\"customer_id\"] == str(client_id)]\n",
    "    if \"apr_effective\" in client_loans.columns and \"apr_annual\" in client_loans.columns:\n",
    "        gap = client_loans[\"apr_annual\"] - client_loans[\"apr_effective\"]\n",
    "        alert = (gap > bps_gap).any()\n",
    "        return {\"alert\": alert, \"gap\": gap}\n",
    "    return {\"alert\": False, \"gap\": None}\n",
    "\n",
    "# --- Example usage for a client (requires invoices, credit_used, credit_limit) ---\n",
    "# invoices: DataFrame with ['customer_id', 'anchor_id', 'invoice_amount']\n",
    "# loans: DataFrame with ['customer_id', 'term_days', 'apr_annual', 'apr_effective']\n",
    "# credit_used, credit_limit: numeric values for the client\n",
    "#\n",
    "# alert1 = anchor_concentration_alert(invoices, client_id)\n",
    "# if alert1[\"alert\"]:\n",
    "#     print(f\"Alert: >{int(alert1['threshold']*100)}% of invoices with anchor {alert1['anchor_id']} ({alert1['share']:.1%}). Recommend diversifying and limiting exposure.\")\n",
    "#\n",
    "# alert2 = credit_line_usage_alert(credit_used, credit_limit)\n",
    "# if alert2[\"alert\"]:\n",
    "#     print(f\"Alert: Credit line usage >{int(alert2['threshold']*100)}% ({alert2['usage']:.1%}). Review capacity and monitor liquidity.\")\n",
    "#\n",
    "# alert3 = extended_term_alert(loans, client_id)\n",
    "# if alert3[\"alert\"]:\n",
    "#     print(f\"Alert: Payment terms >90 days detected. Negotiate shorter terms.\")\n",
    "#     display(alert3[\"long_terms\"][['term_days']])\n",
    "#\n",
    "# alert4 = apr_gap_alert(loans, client_id)\n",
    "# if alert4[\"alert\"]:\n",
    "#     print(f\"Alert: Effective APR >500 bps lower than contractual. Review pricing and adjust rates.\")\n",
    "#     display(alert4[\"gap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c248c",
   "metadata": {},
   "source": [
    "# Credit Policy Guidelines and Actions to Optimize Risk-Return\n",
    "\n",
    "**Advance Rate by Risk:**\n",
    "- AAA/Top Anchors: Advance rate 9095% of invoice value.\n",
    "- Medium risk: Advance 8085%.\n",
    "- High risk/unverified: Advance 7075%.\n",
    "- _Action:_ Automatically adjust advance rate based on anchor/payer and client rating. Retain more for payers/segments with a history of late payments or no track record.\n",
    "\n",
    "**Limits per Anchor (Payer):**\n",
    "- Maximum limit: 20% of the portfolio per anchor; 10% for medium anchors or lower ratings.\n",
    "- _Action:_ Monitor concentration and reject new operations that exceed the threshold, or seek co-financiers/insurance.\n",
    "\n",
    "**Limits per Client (Supplier):**\n",
    "- Maximum limit: 10% of Abaco's equity or an absolute amount based on loss absorption capacity.\n",
    "- Graduation: Lower lines for BC clients and minimums for DE.\n",
    "- _Action:_ Automate alerts and line reviews as balances grow.\n",
    "\n",
    "**Minimum Pricing by Risk Bucket:**\n",
    "- A: Minimum APR 25%, commission 12%.\n",
    "- BC: Minimum APR 30%, standard commission (~2%).\n",
    "- DE: Minimum APR 3540%, commission 3%+.\n",
    "- FH: APR 50% or case by case.\n",
    "- _Action:_ Standardize minimums in the system to avoid price wars and protect margin.\n",
    "\n",
    "**Renewal/Evergreen Policies:**\n",
    "- Mandatory pause after 6 months of continuous use, force liquidation and requalification.\n",
    "- Condition: Keep DSO under control for evergreen continuity.\n",
    "\n",
    "---\n",
    "\n",
    "## Actions by Active Client\n",
    "- **Top Clients:** Controlled upsell, increase line if using <70%, offer better terms, standard monitoring.\n",
    "- **Watchlist Clients:** No upsell, reinforce collections, close monitoring, require monthly financial info.\n",
    "- **Run-off Clients:** No new operations, focus on recovery, document lessons learned.\n",
    "- **New Potentials:** Prospect clones of the best, pre-approve guidelines for ideal profiles.\n",
    "- **Payers with Red Flags:** Limit/discontinue financing, monitor news, restricted lists.\n",
    "\n",
    "---\n",
    "\n",
    "## Ideal Client Profile\n",
    "Medium-sized company, strong financial and credit history, supplier to several large corporates, regular and disciplined use of the line, good documentation and transparency.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "Implementing these policies and actions allows:\n",
    "- Maximizing income with top clients (more safe business, better rates).\n",
    "- Minimizing losses with risky clients (limits, pricing, monitoring).\n",
    "- Strengthening risk culture and portfolio stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34546381",
   "metadata": {},
   "source": [
    "# Net Interest Margin After Loss (NIMAL)\n",
    "\n",
    "**Definition:** Abaco's net financial margin after deducting direct funding costs and credit losses (realized or provisioned).\n",
    "\n",
    "**Formulas:**\n",
    "- **NIMAL (on cost):**\n",
    "  $$(\\text{Net financial income} - \\text{losses}) / \\text{Direct cost}$$\n",
    "- **NIMAL (on AUM):**\n",
    "  $$(\\text{Net financial income} - \\text{losses}) / \\text{Average AUM}$$\n",
    "\n",
    "**Executive Interpretation:**\n",
    "- **2024:** NIMAL after loss (on direct cost): ~80.2%  \n",
    "  For every USD 1 of direct cost (funding + credit), Abaco generates ~0.80 USD of net margin after losses.\n",
    "- **2025 YTD:** NIMAL after loss (on direct cost): ~66.6%  \n",
    "  The margin remains high, but drops vs 2024 due to higher marginal funding cost, scaling of structure, and greater weight of long terms/risk.\n",
    "\n",
    "**Connection with Portfolio and KPIs:**\n",
    "- **NIMAL_AUM:**\n",
    "  $$(\\text{Interest income} + \\text{fees} - \\text{funding cost} - \\text{credit losses}) / \\text{Average AUM}$$\n",
    "- **Relation to Effective APR:**\n",
    "  NIMAL after loss = Effective APR  funding cost  realized loss rate.\n",
    "\n",
    "**For Investors:**\n",
    "- NIMAL is a key KPI to show the real profitability of the portfolio after funding and risk.\n",
    "- 2024: ~80.2% on direct cost.\n",
    "- 2025 YTD: ~66.6% on direct cost.\n",
    "\n",
    "_These ratios are ready to be shown as NIMAL (post-loss) in the deck and can be aligned with the effective APR and other KPIs from your loan tape._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] File not found: ../data/EEFF ABACO CONSOLIDADO - ESTADO DE RESULTADOS - SEP-25.xlsx\n",
      "Please check the path and filename.\n"
     ]
    }
   ],
   "source": [
    "# NIMAL_AUM calculation using EEFF and portfolio data\n",
    "import pandas as pd\n",
    "import os\n",
    "import builtins\n",
    "\n",
    "# Path to consolidated EEFF file (adjust as needed)\n",
    "eeff_path = '../data/EEFF ABACO CONSOLIDADO - ESTADO DE RESULTADOS - SEP-25.xlsx'\n",
    "\n",
    "if not os.path.exists(eeff_path):\n",
    "    builtins.print(f\"[ERROR] File not found: {eeff_path}\\nPlease check the path and filename.\")\n",
    "else:\n",
    "    pyg = pd.read_excel(eeff_path, sheet_name='PYG', header=None)\n",
    "\n",
    "    # Extract income, direct cost, and losses (adjust rows/columns as needed)\n",
    "    # Example: search for row labels if available, else use index\n",
    "    def find_row_index(df, label):\n",
    "        for i, row in df.iterrows():\n",
    "            if builtins.any(label.lower() in str(cell).lower() for cell in row):\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    idx_ingresos = find_row_index(pyg, 'ingreso') or 11\n",
    "    idx_costo = find_row_index(pyg, 'costo') or 12\n",
    "    idx_perdidas = find_row_index(pyg, 'prdida') or 13\n",
    "\n",
    "    ingresos = pyg.iloc[idx_ingresos, 2]\n",
    "    costo_directo = pyg.iloc[idx_costo, 2]\n",
    "    perdidas = pyg.iloc[idx_perdidas, 2]\n",
    "\n",
    "    # Calculate average AUM from available data (preferably from EEFF or loan tape)\n",
    "    # Example: if you have aum values in the EEFF, extract them; else, prompt user\n",
    "    aum_col = None\n",
    "    for col in builtins.range(pyg.shape[1]):\n",
    "        if 'aum' in str(pyg.iloc[0, col]).lower():\n",
    "            aum_col = col\n",
    "            break\n",
    "\n",
    "    if aum_col is not None:\n",
    "        aum_inicio = pyg.iloc[1, aum_col]\n",
    "        aum_fin = pyg.iloc[-1, aum_col]\n",
    "        aum_prom = (aum_inicio + aum_fin) / 2 if pd.notnull(aum_inicio) and pd.notnull(aum_fin) else None\n",
    "    else:\n",
    "        builtins.print('[WARNING] AUM columns not found in EEFF. Please provide actual AUM values.')\n",
    "        aum_prom = None\n",
    "\n",
    "    # NIMAL on cost and on AUM\n",
    "    nimal_costo = (ingresos - perdidas) / costo_directo if costo_directo else None\n",
    "    nimal_aum = (ingresos - perdidas) / aum_prom if aum_prom else None\n",
    "\n",
    "    builtins.print(f\"NIMAL (on direct cost): {nimal_costo:.2%}\" if nimal_costo is not None else '[ERROR] NIMAL (on direct cost) could not be calculated.')\n",
    "    builtins.print(f\"NIMAL (on average AUM): {nimal_aum:.2%}\" if nimal_aum is not None else '[ERROR] NIMAL (on average AUM) could not be calculated.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
